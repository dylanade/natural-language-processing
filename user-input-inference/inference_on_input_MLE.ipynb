{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Student Name**: Dylan Govender\n",
        "#### ***COMP316 - Natural Language Processing***\n",
        "#### ***Assignment II***"
      ],
      "metadata": {
        "id": "f54OutVTH3PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Installing important libraries and modules:***"
      ],
      "metadata": {
        "id": "u8NbQKNDRrbu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfdB78IQQy_l",
        "outputId": "77a1dedd-e160-41a0-92fd-5910070dd8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install dill\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Getting n-gram size and user input:***"
      ],
      "metadata": {
        "id": "eUsoukKZBYsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.util import pad_sequence\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "#user input text\n",
        "text = input(\"User input: \")\n",
        "\n",
        "#user specify n-gram size\n",
        "n = int(input(\"\\nN = \"))\n",
        "\n",
        "#removing start tag and end tag because we can use padding and tokenizing\n",
        "if (('<s>' in text.lower()) or ('</s>' in text.lower())):\n",
        "    text = text.replace('<s>', '')\n",
        "    text = text.replace('</s>', '')\n",
        "\n",
        "#tokenizing the text\n",
        "tokenized_text = [list(map(str, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
        "\n",
        "print(\"\\nUser's tokenized text: \")\n",
        "print(tokenized_text[0])\n",
        "\n",
        "#padding text by adding start tag and end tag\n",
        "padded_text = list(pad_sequence(tokenized_text[0], pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=n))\n",
        "\n",
        "print(\"\\nPadding text: \")\n",
        "print(padded_text)\n",
        "\n",
        "print(\"\\nPadding text into sequence:\")\n",
        "print(list(ngrams(padded_text, n=n)))\n",
        "\n",
        "print(\"\\nFlattening text:\")\n",
        "flattened_text = list(flatten(pad_both_ends(sent, n=n) for sent in tokenized_text))\n",
        "print(flattened_text)\n",
        "\n",
        "print(\"\\nN-Gram(s):\")\n",
        "n_grams = ngrams(flattened_text, n)\n",
        "for n_gram in n_grams:\n",
        "    print(n_gram)\n",
        "\n",
        "#EXAMPLE:\n",
        "#USER INPUT: Language users never choose words randomly, and language is essentially non-random.\n",
        "#USER INPUT: <s> language is never random </s>\n",
        "\n",
        "#Program can work with or without specifying start tag '<s>' or end tag '</s>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QniGqQYlbTaJ",
        "outputId": "1dc250e0-429b-4877-cbce-042e0a1d86e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User input: Language users never choose words randomly, and language is essentially non-random.\n",
            "\n",
            "N = 3\n",
            "\n",
            "User's tokenized text: \n",
            "['Language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n",
            "\n",
            "Padding text: \n",
            "['<s>', '<s>', 'Language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.', '</s>', '</s>']\n",
            "\n",
            "Padding text into sequence:\n",
            "[('<s>', '<s>', 'Language'), ('<s>', 'Language', 'users'), ('Language', 'users', 'never'), ('users', 'never', 'choose'), ('never', 'choose', 'words'), ('choose', 'words', 'randomly'), ('words', 'randomly', ','), ('randomly', ',', 'and'), (',', 'and', 'language'), ('and', 'language', 'is'), ('language', 'is', 'essentially'), ('is', 'essentially', 'non-random'), ('essentially', 'non-random', '.'), ('non-random', '.', '</s>'), ('.', '</s>', '</s>')]\n",
            "\n",
            "Flattening text:\n",
            "['<s>', '<s>', 'Language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.', '</s>', '</s>']\n",
            "\n",
            "N-Gram(s):\n",
            "('<s>', '<s>', 'Language')\n",
            "('<s>', 'Language', 'users')\n",
            "('Language', 'users', 'never')\n",
            "('users', 'never', 'choose')\n",
            "('never', 'choose', 'words')\n",
            "('choose', 'words', 'randomly')\n",
            "('words', 'randomly', ',')\n",
            "('randomly', ',', 'and')\n",
            "(',', 'and', 'language')\n",
            "('and', 'language', 'is')\n",
            "('language', 'is', 'essentially')\n",
            "('is', 'essentially', 'non-random')\n",
            "('essentially', 'non-random', '.')\n",
            "('non-random', '.', '</s>')\n",
            "('.', '</s>', '</s>')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Initialising a corpus:***"
      ],
      "metadata": {
        "id": "7_fHjw1fJXIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import requests\n",
        "\n",
        "#Text version: https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
        "#Example from website: https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk\n",
        "\n",
        "if os.path.isfile('language-never-random.txt'):\n",
        "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
        "        corpus = fin.read();\n",
        "else:\n",
        "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
        "    corpus = requests.get(url).content.decode('utf8')\n",
        "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "        fout.write(corpus)\n",
        "\n",
        "print(\"Part of corpus:\")\n",
        "print(corpus[:979])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBWJOKSCJUfj",
        "outputId": "2548d21c-0e04-49e3-850d-dfe55e1cb036"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of corpus:\n",
            "                       Language is never, ever, ever, random\n",
            "\n",
            "                                                               ADAM KILGARRIFF\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Abstract\n",
            "Language users never choose words randomly, and language is essentially\n",
            "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
            "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
            "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
            "data, we shall (almost) always be able to establish that it is not true. In\n",
            "corpus studies, we frequently do have enough data, so the fact that a rela-\n",
            "tion between two phenomena is demonstrably non-random, does not sup-\n",
            "port the inference that it is not arbitrary. We present experimental evidence\n",
            "of how arbitrary associations between word frequencies and corpora are\n",
            "systematically non-random. We review literature in which hypothesis test-\n",
            "ing has been used, and show how it has often led to unhelpful or mislead-\n",
            "ing results.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Processing the data from the corpus:***"
      ],
      "metadata": {
        "id": "MEU06yXOJLI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "#tokenizing the corpus\n",
        "tokenized_corpus = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(corpus)]\n",
        "\n",
        "#display tokenized corpus first sentence\n",
        "print(\"\\nTokenized corpus: \")\n",
        "print(tokenized_corpus[0])\n",
        "\n",
        "#pipeline the corpus\n",
        "print(\"\\nPipelining corpus:\")\n",
        "training, padding = padded_everygram_pipeline(n, tokenized_corpus)\n",
        "\n",
        "#display pipeline(s)\n",
        "count = 0;\n",
        "for ngram in training:\n",
        "    count = count + 1\n",
        "    if (count <= 10):\n",
        "        print(list(ngram))\n",
        "\n",
        "#display number of pipeline sequence(s)\n",
        "print(\"\\n\"  + str(count) + \" total pipelined sequence(s).\") #Too many to display - will just output some of the pipeline sequence(s)\n",
        "\n",
        "print(\"\\nCorpus vocabulary:\")\n",
        "print(list(padding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcbaDjtPI7Zp",
        "outputId": "c6406616-c517-4a72-8136-94f7328a8eff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized corpus: \n",
            "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n",
            "\n",
            "Pipelining corpus:\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'language'), ('<s>',), ('<s>', 'language'), ('<s>', 'language', 'is'), ('language',), ('language', 'is'), ('language', 'is', 'never'), ('is',), ('is', 'never'), ('is', 'never', ','), ('never',), ('never', ','), ('never', ',', 'ever'), (',',), (',', 'ever'), (',', 'ever', ','), ('ever',), ('ever', ','), ('ever', ',', 'ever'), (',',), (',', 'ever'), (',', 'ever', ','), ('ever',), ('ever', ','), ('ever', ',', 'random'), (',',), (',', 'random'), (',', 'random', 'adam'), ('random',), ('random', 'adam'), ('random', 'adam', 'kilgarriff'), ('adam',), ('adam', 'kilgarriff'), ('adam', 'kilgarriff', 'abstract'), ('kilgarriff',), ('kilgarriff', 'abstract'), ('kilgarriff', 'abstract', 'language'), ('abstract',), ('abstract', 'language'), ('abstract', 'language', 'users'), ('language',), ('language', 'users'), ('language', 'users', 'never'), ('users',), ('users', 'never'), ('users', 'never', 'choose'), ('never',), ('never', 'choose'), ('never', 'choose', 'words'), ('choose',), ('choose', 'words'), ('choose', 'words', 'randomly'), ('words',), ('words', 'randomly'), ('words', 'randomly', ','), ('randomly',), ('randomly', ','), ('randomly', ',', 'and'), (',',), (',', 'and'), (',', 'and', 'language'), ('and',), ('and', 'language'), ('and', 'language', 'is'), ('language',), ('language', 'is'), ('language', 'is', 'essentially'), ('is',), ('is', 'essentially'), ('is', 'essentially', 'non-random'), ('essentially',), ('essentially', 'non-random'), ('essentially', 'non-random', '.'), ('non-random',), ('non-random', '.'), ('non-random', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'statistical'), ('<s>',), ('<s>', 'statistical'), ('<s>', 'statistical', 'hypothesis'), ('statistical',), ('statistical', 'hypothesis'), ('statistical', 'hypothesis', 'testing'), ('hypothesis',), ('hypothesis', 'testing'), ('hypothesis', 'testing', 'uses'), ('testing',), ('testing', 'uses'), ('testing', 'uses', 'a'), ('uses',), ('uses', 'a'), ('uses', 'a', 'null'), ('a',), ('a', 'null'), ('a', 'null', 'hypothesis'), ('null',), ('null', 'hypothesis'), ('null', 'hypothesis', ','), ('hypothesis',), ('hypothesis', ','), ('hypothesis', ',', 'which'), (',',), (',', 'which'), (',', 'which', 'posits'), ('which',), ('which', 'posits'), ('which', 'posits', 'randomness'), ('posits',), ('posits', 'randomness'), ('posits', 'randomness', '.'), ('randomness',), ('randomness', '.'), ('randomness', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'hence'), ('<s>',), ('<s>', 'hence'), ('<s>', 'hence', ','), ('hence',), ('hence', ','), ('hence', ',', 'when'), (',',), (',', 'when'), (',', 'when', 'we'), ('when',), ('when', 'we'), ('when', 'we', 'look'), ('we',), ('we', 'look'), ('we', 'look', 'at'), ('look',), ('look', 'at'), ('look', 'at', 'linguistic'), ('at',), ('at', 'linguistic'), ('at', 'linguistic', 'phenomena'), ('linguistic',), ('linguistic', 'phenomena'), ('linguistic', 'phenomena', 'in'), ('phenomena',), ('phenomena', 'in'), ('phenomena', 'in', 'cor-'), ('in',), ('in', 'cor-'), ('in', 'cor-', 'pora'), ('cor-',), ('cor-', 'pora'), ('cor-', 'pora', ','), ('pora',), ('pora', ','), ('pora', ',', 'the'), (',',), (',', 'the'), (',', 'the', 'null'), ('the',), ('the', 'null'), ('the', 'null', 'hypothesis'), ('null',), ('null', 'hypothesis'), ('null', 'hypothesis', 'will'), ('hypothesis',), ('hypothesis', 'will'), ('hypothesis', 'will', 'never'), ('will',), ('will', 'never'), ('will', 'never', 'be'), ('never',), ('never', 'be'), ('never', 'be', 'true'), ('be',), ('be', 'true'), ('be', 'true', '.'), ('true',), ('true', '.'), ('true', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'moreover'), ('<s>',), ('<s>', 'moreover'), ('<s>', 'moreover', ','), ('moreover',), ('moreover', ','), ('moreover', ',', 'where'), (',',), (',', 'where'), (',', 'where', 'there'), ('where',), ('where', 'there'), ('where', 'there', 'is'), ('there',), ('there', 'is'), ('there', 'is', 'enough'), ('is',), ('is', 'enough'), ('is', 'enough', 'data'), ('enough',), ('enough', 'data'), ('enough', 'data', ','), ('data',), ('data', ','), ('data', ',', 'we'), (',',), (',', 'we'), (',', 'we', 'shall'), ('we',), ('we', 'shall'), ('we', 'shall', '('), ('shall',), ('shall', '('), ('shall', '(', 'almost'), ('(',), ('(', 'almost'), ('(', 'almost', ')'), ('almost',), ('almost', ')'), ('almost', ')', 'always'), (')',), (')', 'always'), (')', 'always', 'be'), ('always',), ('always', 'be'), ('always', 'be', 'able'), ('be',), ('be', 'able'), ('be', 'able', 'to'), ('able',), ('able', 'to'), ('able', 'to', 'establish'), ('to',), ('to', 'establish'), ('to', 'establish', 'that'), ('establish',), ('establish', 'that'), ('establish', 'that', 'it'), ('that',), ('that', 'it'), ('that', 'it', 'is'), ('it',), ('it', 'is'), ('it', 'is', 'not'), ('is',), ('is', 'not'), ('is', 'not', 'true'), ('not',), ('not', 'true'), ('not', 'true', '.'), ('true',), ('true', '.'), ('true', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'in'), ('<s>',), ('<s>', 'in'), ('<s>', 'in', 'corpus'), ('in',), ('in', 'corpus'), ('in', 'corpus', 'studies'), ('corpus',), ('corpus', 'studies'), ('corpus', 'studies', ','), ('studies',), ('studies', ','), ('studies', ',', 'we'), (',',), (',', 'we'), (',', 'we', 'frequently'), ('we',), ('we', 'frequently'), ('we', 'frequently', 'do'), ('frequently',), ('frequently', 'do'), ('frequently', 'do', 'have'), ('do',), ('do', 'have'), ('do', 'have', 'enough'), ('have',), ('have', 'enough'), ('have', 'enough', 'data'), ('enough',), ('enough', 'data'), ('enough', 'data', ','), ('data',), ('data', ','), ('data', ',', 'so'), (',',), (',', 'so'), (',', 'so', 'the'), ('so',), ('so', 'the'), ('so', 'the', 'fact'), ('the',), ('the', 'fact'), ('the', 'fact', 'that'), ('fact',), ('fact', 'that'), ('fact', 'that', 'a'), ('that',), ('that', 'a'), ('that', 'a', 'rela-'), ('a',), ('a', 'rela-'), ('a', 'rela-', 'tion'), ('rela-',), ('rela-', 'tion'), ('rela-', 'tion', 'between'), ('tion',), ('tion', 'between'), ('tion', 'between', 'two'), ('between',), ('between', 'two'), ('between', 'two', 'phenomena'), ('two',), ('two', 'phenomena'), ('two', 'phenomena', 'is'), ('phenomena',), ('phenomena', 'is'), ('phenomena', 'is', 'demonstrably'), ('is',), ('is', 'demonstrably'), ('is', 'demonstrably', 'non-random'), ('demonstrably',), ('demonstrably', 'non-random'), ('demonstrably', 'non-random', ','), ('non-random',), ('non-random', ','), ('non-random', ',', 'does'), (',',), (',', 'does'), (',', 'does', 'not'), ('does',), ('does', 'not'), ('does', 'not', 'sup-'), ('not',), ('not', 'sup-'), ('not', 'sup-', 'port'), ('sup-',), ('sup-', 'port'), ('sup-', 'port', 'the'), ('port',), ('port', 'the'), ('port', 'the', 'inference'), ('the',), ('the', 'inference'), ('the', 'inference', 'that'), ('inference',), ('inference', 'that'), ('inference', 'that', 'it'), ('that',), ('that', 'it'), ('that', 'it', 'is'), ('it',), ('it', 'is'), ('it', 'is', 'not'), ('is',), ('is', 'not'), ('is', 'not', 'arbitrary'), ('not',), ('not', 'arbitrary'), ('not', 'arbitrary', '.'), ('arbitrary',), ('arbitrary', '.'), ('arbitrary', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'we'), ('<s>',), ('<s>', 'we'), ('<s>', 'we', 'present'), ('we',), ('we', 'present'), ('we', 'present', 'experimental'), ('present',), ('present', 'experimental'), ('present', 'experimental', 'evidence'), ('experimental',), ('experimental', 'evidence'), ('experimental', 'evidence', 'of'), ('evidence',), ('evidence', 'of'), ('evidence', 'of', 'how'), ('of',), ('of', 'how'), ('of', 'how', 'arbitrary'), ('how',), ('how', 'arbitrary'), ('how', 'arbitrary', 'associations'), ('arbitrary',), ('arbitrary', 'associations'), ('arbitrary', 'associations', 'between'), ('associations',), ('associations', 'between'), ('associations', 'between', 'word'), ('between',), ('between', 'word'), ('between', 'word', 'frequencies'), ('word',), ('word', 'frequencies'), ('word', 'frequencies', 'and'), ('frequencies',), ('frequencies', 'and'), ('frequencies', 'and', 'corpora'), ('and',), ('and', 'corpora'), ('and', 'corpora', 'are'), ('corpora',), ('corpora', 'are'), ('corpora', 'are', 'systematically'), ('are',), ('are', 'systematically'), ('are', 'systematically', 'non-random'), ('systematically',), ('systematically', 'non-random'), ('systematically', 'non-random', '.'), ('non-random',), ('non-random', '.'), ('non-random', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'we'), ('<s>',), ('<s>', 'we'), ('<s>', 'we', 'review'), ('we',), ('we', 'review'), ('we', 'review', 'literature'), ('review',), ('review', 'literature'), ('review', 'literature', 'in'), ('literature',), ('literature', 'in'), ('literature', 'in', 'which'), ('in',), ('in', 'which'), ('in', 'which', 'hypothesis'), ('which',), ('which', 'hypothesis'), ('which', 'hypothesis', 'test-'), ('hypothesis',), ('hypothesis', 'test-'), ('hypothesis', 'test-', 'ing'), ('test-',), ('test-', 'ing'), ('test-', 'ing', 'has'), ('ing',), ('ing', 'has'), ('ing', 'has', 'been'), ('has',), ('has', 'been'), ('has', 'been', 'used'), ('been',), ('been', 'used'), ('been', 'used', ','), ('used',), ('used', ','), ('used', ',', 'and'), (',',), (',', 'and'), (',', 'and', 'show'), ('and',), ('and', 'show'), ('and', 'show', 'how'), ('show',), ('show', 'how'), ('show', 'how', 'it'), ('how',), ('how', 'it'), ('how', 'it', 'has'), ('it',), ('it', 'has'), ('it', 'has', 'often'), ('has',), ('has', 'often'), ('has', 'often', 'led'), ('often',), ('often', 'led'), ('often', 'led', 'to'), ('led',), ('led', 'to'), ('led', 'to', 'unhelpful'), ('to',), ('to', 'unhelpful'), ('to', 'unhelpful', 'or'), ('unhelpful',), ('unhelpful', 'or'), ('unhelpful', 'or', 'mislead-'), ('or',), ('or', 'mislead-'), ('or', 'mislead-', 'ing'), ('mislead-',), ('mislead-', 'ing'), ('mislead-', 'ing', 'results'), ('ing',), ('ing', 'results'), ('ing', 'results', '.'), ('results',), ('results', '.'), ('results', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'keywords'), ('<s>',), ('<s>', 'keywords'), ('<s>', 'keywords', ':'), ('keywords',), ('keywords', ':'), ('keywords', ':', '쎲쎲쎲'), (':',), (':', '쎲쎲쎲'), (':', '쎲쎲쎲', '1'), ('쎲쎲쎲',), ('쎲쎲쎲', '1'), ('쎲쎲쎲', '1', '.'), ('1',), ('1', '.'), ('1', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'introduction'), ('<s>',), ('<s>', 'introduction'), ('<s>', 'introduction', 'any'), ('introduction',), ('introduction', 'any'), ('introduction', 'any', 'two'), ('any',), ('any', 'two'), ('any', 'two', 'phenomena'), ('two',), ('two', 'phenomena'), ('two', 'phenomena', 'might'), ('phenomena',), ('phenomena', 'might'), ('phenomena', 'might', 'or'), ('might',), ('might', 'or'), ('might', 'or', 'might'), ('or',), ('or', 'might'), ('or', 'might', 'not'), ('might',), ('might', 'not'), ('might', 'not', 'be'), ('not',), ('not', 'be'), ('not', 'be', 'related'), ('be',), ('be', 'related'), ('be', 'related', '.'), ('related',), ('related', '.'), ('related', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'the'), ('<s>',), ('<s>', 'the'), ('<s>', 'the', 'range'), ('the',), ('the', 'range'), ('the', 'range', 'of'), ('range',), ('range', 'of'), ('range', 'of', 'pos-'), ('of',), ('of', 'pos-'), ('of', 'pos-', 'sibilities'), ('pos-',), ('pos-', 'sibilities'), ('pos-', 'sibilities', 'is'), ('sibilities',), ('sibilities', 'is'), ('sibilities', 'is', 'that'), ('is',), ('is', 'that'), ('is', 'that', 'the'), ('that',), ('that', 'the'), ('that', 'the', 'association'), ('the',), ('the', 'association'), ('the', 'association', 'is'), ('association',), ('association', 'is'), ('association', 'is', 'random'), ('is',), ('is', 'random'), ('is', 'random', ','), ('random',), ('random', ','), ('random', ',', 'arbitrary'), (',',), (',', 'arbitrary'), (',', 'arbitrary', ','), ('arbitrary',), ('arbitrary', ','), ('arbitrary', ',', 'motivated'), (',',), (',', 'motivated'), (',', 'motivated', 'or'), ('motivated',), ('motivated', 'or'), ('motivated', 'or', 'pre-'), ('or',), ('or', 'pre-'), ('or', 'pre-', 'dictable'), ('pre-',), ('pre-', 'dictable'), ('pre-', 'dictable', '('), ('dictable',), ('dictable', '('), ('dictable', '(', 'r'), ('(',), ('(', 'r'), ('(', 'r', ','), ('r',), ('r', ','), ('r', ',', 'a'), (',',), (',', 'a'), (',', 'a', ','), ('a',), ('a', ','), ('a', ',', 'm'), (',',), (',', 'm'), (',', 'm', ','), ('m',), ('m', ','), ('m', ',', 'p'), (',',), (',', 'p'), (',', 'p', ')'), ('p',), ('p', ')'), ('p', ')', '.'), (')',), (')', '.'), (')', '.', '</s>'), ('.',), ('.', '</s>'), ('.', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
            "\n",
            "235 total pipelined sequence(s).\n",
            "\n",
            "Corpus vocabulary:\n",
            "['<s>', '<s>', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.', '</s>', '</s>', '<s>', '<s>', 'statistical', 'hypothesis', 'testing', 'uses', 'a', 'null', 'hypothesis', ',', 'which', 'posits', 'randomness', '.', '</s>', '</s>', '<s>', '<s>', 'hence', ',', 'when', 'we', 'look', 'at', 'linguistic', 'phenomena', 'in', 'cor-', 'pora', ',', 'the', 'null', 'hypothesis', 'will', 'never', 'be', 'true', '.', '</s>', '</s>', '<s>', '<s>', 'moreover', ',', 'where', 'there', 'is', 'enough', 'data', ',', 'we', 'shall', '(', 'almost', ')', 'always', 'be', 'able', 'to', 'establish', 'that', 'it', 'is', 'not', 'true', '.', '</s>', '</s>', '<s>', '<s>', 'in', 'corpus', 'studies', ',', 'we', 'frequently', 'do', 'have', 'enough', 'data', ',', 'so', 'the', 'fact', 'that', 'a', 'rela-', 'tion', 'between', 'two', 'phenomena', 'is', 'demonstrably', 'non-random', ',', 'does', 'not', 'sup-', 'port', 'the', 'inference', 'that', 'it', 'is', 'not', 'arbitrary', '.', '</s>', '</s>', '<s>', '<s>', 'we', 'present', 'experimental', 'evidence', 'of', 'how', 'arbitrary', 'associations', 'between', 'word', 'frequencies', 'and', 'corpora', 'are', 'systematically', 'non-random', '.', '</s>', '</s>', '<s>', '<s>', 'we', 'review', 'literature', 'in', 'which', 'hypothesis', 'test-', 'ing', 'has', 'been', 'used', ',', 'and', 'show', 'how', 'it', 'has', 'often', 'led', 'to', 'unhelpful', 'or', 'mislead-', 'ing', 'results', '.', '</s>', '</s>', '<s>', '<s>', 'keywords', ':', '쎲쎲쎲', '1', '.', '</s>', '</s>', '<s>', '<s>', 'introduction', 'any', 'two', 'phenomena', 'might', 'or', 'might', 'not', 'be', 'related', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'range', 'of', 'pos-', 'sibilities', 'is', 'that', 'the', 'association', 'is', 'random', ',', 'arbitrary', ',', 'motivated', 'or', 'pre-', 'dictable', '(', 'r', ',', 'a', ',', 'm', ',', 'p', ')', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'bulk', 'of', 'linguistic', 'questions', 'concern', 'the', 'dis-', 'tinction', 'between', 'a', 'and', 'm.', 'a', 'linguistic', 'account', 'of', 'a', 'phenomenon', 'gen-', 'erally', 'gives', 'us', 'reason', 'to', 'view', 'the', 'relation', 'between', ',', 'for', 'example', ',', 'a', 'verb', '’', 's', 'syntax', 'and', 'its', 'semantics', ',', 'as', 'motivated', 'rather', 'than', 'arbitrary', '.', '</s>', '</s>', '<s>', '<s>', 'however', ',', 'it', 'is', 'not', 'in', 'general', 'possible', 'to', 'model', 'the', 'a-m', 'distinction', 'mathematically', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'distinction', 'that', 'can', 'be', 'modeled', 'mathematically', 'is', 'between', 'r', 'and', 'not-r', ',', 'that', 'is', ',', 'between', 'random', ',', 'or', 'uncorrelated', ',', 'pairs', 'and', 'pairs', 'where', 'there', 'is', 'some', 'correlation', ',', 'be', 'it', 'arbitrary', ',', 'motivated', 'or', 'predictable.1', 'the', 'mechanism', 'here', 'is', 'hypothesis-testing', '.', '</s>', '</s>', '<s>', '<s>', 'a', 'null', 'hypothesis', ',', 'h0', 'is', 'con-', 'structed', 'to', 'model', 'the', 'situation', 'in', 'which', 'there', 'is', 'no', 'correlation', 'between', 'corpus', 'linguistics', 'and', 'linguistic', 'theory', '1⫺2', '(', '2005', ')', ',', '263⫺275', '1613-7027/05/0001⫺0263', '쑕', 'walter', 'de', 'gruyter', '264', 'a.', 'kilgarriff', 'the', 'two', 'phenomena', '.', '</s>', '</s>', '<s>', '<s>', 'as', 'the', 'mathematics', 'of', 'the', 'random', 'is', 'well', 'under-', 'stood', ',', 'we', 'can', 'compute', 'the', 'likelihood', 'of', 'the', 'null', 'hypothesis', 'given', 'the', 'data', '.', '</s>', '</s>', '<s>', '<s>', 'if', 'the', 'likelihood', 'is', 'low', ',', 'we', 'reject', 'h0', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'problem', 'for', 'empirical', 'linguistics', 'is', 'that', 'language', 'is', 'not', 'random', ',', 'so', 'the', 'null', 'hypothesis', 'is', 'never', 'true', '.', '</s>', '</s>', '<s>', '<s>', 'language', 'is', 'not', 'random', 'because', 'we', 'speak', 'or', 'write', 'with', 'purposes', '.', '</s>', '</s>', '<s>', '<s>', 'we', 'do', 'not', ',', 'indeed', ',', 'without', 'computational', 'help', 'are', 'not', 'capable', 'of', ',', 'producing', 'words', 'or', 'sounds', 'or', 'sentences', 'or', 'documents', 'randomly', '.', '</s>', '</s>', '<s>', '<s>', 'we', 'do', 'not', 'always', 'have', 'enough', 'data', 'to', 'reject', 'the', 'null', 'hypothesis', ',', 'but', 'that', 'is', 'a', 'distinct', 'issue', ':', 'wherever', 'there', 'is', 'enough', 'data', ',', 'it', 'is', 'rejected', '.', '</s>', '</s>', '<s>', '<s>', 'using', 'language', 'corpora', ',', 'we', 'are', 'frequently', 'in', 'the', 'fortunate', 'position', 'of', 'having', 'very', 'large', 'quantities', 'of', 'data', 'at', 'our', 'disposal', '.', '</s>', '</s>', '<s>', '<s>', 'then', ',', 'even', 'where', 'pairs', 'of', 'corpora', 'are', 'set', 'up', 'to', 'be', 'linguistically', 'identical', ',', 'the', 'null', 'hypothesis', 'is', 'resoundingly', 'defeated', '.', '</s>', '</s>', '<s>', '<s>', 'in', 'section', '4', ',', 'we', 'present', 'an', 'experiment', 'demonstrating', 'this', 'counterintuitive', 'effect', '.', '</s>', '</s>', '<s>', '<s>', 'there', 'are', 'a', 'number', 'of', 'papers', 'in', 'the', 'empirical', 'linguistics', 'literature', 'where', 'researchers', 'seemed', 'to', 'be', 'testing', 'whether', 'an', 'association', 'was', 'lin-', 'guistically', 'salient', ',', 'or', 'used', 'the', 'confidence', 'with', 'which', 'h0', 'could', 'be', 're-', 'jected', 'as', 'a', 'measure', 'of', 'salience', ',', 'whereas', 'in', 'fact', 'they', 'were', 'merely', 'testing', 'whether', 'they', 'had', 'enough', 'data', 'to', 'reject', 'h0', 'with', 'confidence', '.', '</s>', '</s>', '<s>', '<s>', 'some', 'such', 'cases', 'are', 'reviewed', 'in', 'section', '5', '.', '</s>', '</s>', '<s>', '<s>', 'hypothesis', 'testing', 'has', 'been', 'widely', 'used', 'in', 'the', 'acquisition', 'of', 'subcategorization', 'frames', 'from', 'corpora', 'and', 'this', 'literature', 'is', 'considered', 'in', 'some', 'detail', '.', '</s>', '</s>', '<s>', '<s>', 'alternatives', 'to', 'inappropriate', 'hy-', 'pothesis-testing', 'are', 'presented', '.', '</s>', '</s>', '<s>', '<s>', 'before', 'proceeding', ',', 'may', 'i', 'clarify', 'that', 'this', 'paper', 'is', 'in', 'no', 'way', 'critical', 'of', 'using', 'probability', 'models', ',', 'all', 'of', 'which', 'are', 'based', 'on', 'assumptions', 'of', 'randomness', ',', 'in', 'empirical', 'linguistics', 'in', 'general', '.', '</s>', '</s>', '<s>', '<s>', 'probability', 'models', 'have', 'been', 'responsible', 'for', 'a', 'large', 'share', 'of', 'progress', 'in', 'the', 'field', 'in', 'the', 'last', 'decade', 'and', 'a', 'half', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'randomness', 'assumptions', 'are', 'always', 'untrue', ',', 'but', 'that', 'does', 'not', 'preclude', 'them', 'from', 'frequently', 'being', 'useful', '.', '</s>', '</s>', '<s>', '<s>', 'making', 'false', 'assumptions', 'is', 'often', 'an', 'ingenious', 'way', 'to', 'proceed', ';', 'the', 'problem', 'arises', 'where', 'the', 'literal', 'falsity', 'of', 'the', 'assumption', 'is', 'overlooked', ',', 'and', 'inappropri-', 'ate', 'inferences', 'are', 'drawn', '.', '</s>', '</s>', '<s>', '<s>', '2', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'arbitrary', 'and', 'the', 'random', 'in', 'common', 'parlance', ',', 'random', 'and', 'arbitrary', 'are', 'synonyms', ',', 'with', 'diction-', 'aries', 'giving', 'near-identical', 'definitions', ':', 'ldoce', '(', '1995', ')', 'defines', 'random', 'as', 'happening', 'or', 'chosen', 'without', 'any', 'definite', 'plan', ',', 'or', 'pattern', 'and', 'arbitrary', 'as', '1', 'decided', 'or', 'arranged', 'without', 'any', 'reason', 'or', 'plan', ',', 'often', 'unfairly', '…', '2', 'happening', 'or', 'decided', 'by', 'chance', 'rather', 'than', 'a', 'plan', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '265', 'superficially', ',', 'randomness', ',', 'as', 'defined', 'here', ',', 'is', 'what', 'the', 'technical', 'sense', 'of', 'random', 'captures', 'and', 'makes', 'explicit', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'technical', 'sense', 'is', 'defined', 'in', 'terms', 'of', 'statistical', 'independence', '.', '</s>', '</s>', '<s>', '<s>', 'first', ',', 'we', 'formalize', 'the', 'framework', ':', 'for', 'a', 'population', 'of', 'events', ',', 'the', 'first', 'phenomenon', 'holds', 'where', 'x', 'is', 'true', 'of', 'the', 'event', ',', 'the', 'second', 'holds', 'where', 'y', 'is', 'true', 'of', 'the', 'event', '.', '</s>', '</s>', '<s>', '<s>', 'now', ',', 'the', 'relation', 'between', 'the', 'phenomena', 'is', 'random', 'iff', 'the', 'prob-', 'ability', 'of', 'x', ',', 'for', 'that', 'subset', 'of', 'events', 'where', 'y', 'does', 'hold', ',', 'is', 'identical', 'to', 'its', 'probability', 'for', 'the', 'subset', 'where', 'y', 'does', 'not', 'hold', ',', 'that', 'is', 'p', '(', 'x|y', ')', '⫽', 'p', '(', 'x|ÿ', 'y', ')', 'the', 'relation', 'is', 'symmetric', ':', 'p', '(', 'x|y', ')', '⫽', 'p', '(', 'x|ÿ', 'y', ')', 'entails', 'p', '(', 'y|x', ')', '⫽', 'p', '(', 'y|ÿ', 'x', ')', '.', '</s>', '</s>', '<s>', '<s>', 'hereafter', 'i', 'use', '‘', 'random', '’', 'for', 'the', 'technical', 'meaning', 'and', '‘', 'arbi-', 'trary', '’', 'for', 'the', 'non-technical', 'one', '.', '</s>', '</s>', '<s>', '<s>', 'arbitrary', 'events', 'are', 'very', 'rarely', 'random', ',', 'and', 'random', 'events', 'are', 'very', 'rarely', 'arbitrary', '.', '</s>', '</s>', '<s>', '<s>', 'it', 'takes', 'considerable', 'ingenuity', 'and', 'sophisticated', 'mathe-', 'matics', 'to', 'produce', 'a', 'pseudo-random', 'sequence', 'algorithmically', ',', 'and', 'true', 'randomness', 'is', 'not', 'possible', 'at', 'all', '.', '</s>', '</s>', '<s>', '<s>', 'events', 'happening', '“', 'without', 'any', 'defi-', 'nite', 'plan', ',', 'aim', 'or', 'pattern', '”', 'are', ',', 'by', 'definition', ',', 'arbitrary', ',', 'but', 'are', 'vanish-', 'ingly', 'unlikely', 'to', 'be', 'random', '.', '</s>', '</s>', '<s>', '<s>', 'outside', 'the', 'sub-atomic', 'realm', ',', 'natural', 'events', 'are', 'very', 'rarely', 'random', '.', '</s>', '</s>', '<s>', '<s>', 'consider', ',', 'for', 'example', ',', 'cat', 'food', 'purchases', 'and', 'shoe-polish', 'purchases', 'within', 'the', 'space', 'of', 'all', 'uk', 'supermarket-shopping', 'events', ':', 'does', 'the', 'fact', 'that', 'cat', 'food', 'was', 'bought', 'predict', '(', 'positively', 'or', 'negatively', ')', 'whether', 'shoe', 'polish', 'was', 'bought', 'in', 'the', 'same', 'shopping', 'trip', '?', '</s>', '</s>', '<s>', '<s>', 'there', 'is', 'no', 'obvious', 'reason', 'why', 'it', 'should', ',', 'and', 'we', 'can', 'happily', 'declare', 'the', 'relation', 'arbitrary', '.', '</s>', '</s>', '<s>', '<s>', 'but', 'perhaps', 'either', 'cat', 'food', 'or', 'shoe-polish', 'are', 'more', '(', 'or', 'less', ')', 'often', 'bought', 'in', 'hot', '(', 'or', 'cold', ')', 'weather', ',', 'or', 'on', 'saturday', 'nights', ',', 'or', 'sunday', 'mornings', ',', 'or', 'monday', 'lunchtimes', ',', 'or', 'by', 'richer', '(', 'or', 'poorer', ')', 'people', ',', 'or', 'by', 'men', '(', 'or', 'women', ')', ',', 'or', 'by', 'people', 'in', '(', 'or', 'out', 'of', ')', 'towns…', 'there', 'is', 'an', 'unlimited', 'number', 'of', 'hypotheses', 'connecting', 'the', 'two', '(', 'positively', 'or', 'negatively', ')', ';', 'if', 'just', 'one', 'of', 'these', 'has', 'any', 'validity', ',', 'however', 'weak', ',', 'then', 'the', 'null', 'hypothesis', 'is', 'false', '.', '</s>', '</s>', '<s>', '<s>', 'at', 'this', 'point', ',', 'you', 'may', 'question', 'why', 'the', 'null', 'hypothesis', 'is', 'ever', 'a', 'useful', 'construct', '.', '</s>', '</s>', '<s>', '<s>', 'for', 'a', 'wide', 'range', 'of', 'tasks', ',', 'although', 'h0', 'is', 'false', ',', 'there', 'is', 'only', 'enough', 'evidence', 'to', 'establish', 'the', 'fact', 'if', 'there', 'is', 'a', 'strong', 'relation', 'between', 'the', 'two', 'phenomena', '.', '</s>', '</s>', '<s>', '<s>', 'thus', ',', 'given', 'evidence', 'from', '1,000', 'shopping', 'trips', ',', 'it', 'is', 'un-', 'likely', 'we', 'shall', 'be', 'able', 'to', 'reject', 'h0', 'concerning', 'cat', 'food', 'and', 'shoe-polish', ',', 'whereas', 'we', 'shall', 'be', 'able', 'to', 'reject', 'it', 'concerning', 'strawberry-buying', 'and', 'cream-buying', '.', '</s>', '</s>', '<s>', '<s>', 'given', 'further', 'evidence', ',', 'perhaps', 'from', '1,000,000', 'shopping', '266', 'a.', 'kilgarriff', 'trips', ',', 'we', 'shall', 'also', 'be', 'able', 'to', 'reject', 'the', 'null', 'hypothesis', 'regarding', 'nappy2-', 'buying', 'and', 'beer-sixpack-buying', '.', '</s>', '</s>', '<s>', '<s>', '(', 'the', 'correlation', ',', 'the', 'most', 'newsworthy', 'product', 'of', 'large-scale', 'data', 'mining', 'by', 'supermarkets', ',', 'was', 'widely', 'reported', 'in', 'the', 'british', 'media', '.', ')', '</s>', '</s>', '<s>', '<s>', 'but', 'still', 'not', 'for', 'catf', 'ood', 'and', 'shoe-polish', '.', '</s>', '</s>', '<s>', '<s>', 'but', ',', 'given', '1,000,000,000', 'events', ',', 'we', 'shall', 'in', 'all', 'likelihood', 'also', 'be', 'able', 'to', 'reject', 'it', 'for', 'cat', 'food', 'and', 'shoe-polish', '.', '</s>', '</s>', '<s>', '<s>', 'whether', 'or', 'not', 'we', 'can', 'reject', 'the', 'null', 'hypothesis', '(', 'with', 'eg', '.', '</s>', '</s>', '<s>', '<s>', '95', '%', 'confi-', 'dence', ')', 'is', 'a', 'function', 'of', 'sample', 'size', 'and', 'level', 'of', 'correlation', '.', '</s>', '</s>', '<s>', '<s>', 'where', 'sample', 'size', 'is', 'held', 'constant', '(', 'and', 'is', 'not', 'enormous', ')', ',', 'whether', 'or', 'not', 'we', 'can', 'reject', 'h0', 'can', 'be', 'seen', 'as', 'a', 'way', 'of', 'providing', 'statistical', 'support', 'for', 'distinguish-', 'ing', 'the', 'arbitrary', 'and', 'the', 'motivated', '.', '</s>', '</s>', '<s>', '<s>', 'this', 'is', 'a', 'role', 'that', 'hypothesis', 'testing', 'plays', 'across', 'the', 'social', 'sciences', '.', '</s>', '</s>', '<s>', '<s>', 'however', 'where', 'the', 'sample', 'size', 'varies', 'by', 'an', 'order', 'of', 'magnitude', ',', 'or', 'where', 'it', 'is', 'enormous', ',', 'it', 'is', 'wrong', 'to', 'identify', 'the', 'accept-h0/reject-h0', 'distinction', 'with', 'the', 'arbitrary/motivated', 'one', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'uneasy', 'relationship', 'between', 'hypothesis-testing', ',', 'and', 'quantity', 'of', 'data', ',', 'is', 'familiar', 'to', 'statisticians', 'though', 'frequently', 'overlooked', 'or', 'mis-', 'understood', 'by', 'users', 'of', 'statistics', '(', 'carver', '1993', ',', 'stubbs', '1995', ',', 'brandstätter', '1999', ')', '.', '</s>', '</s>', '<s>', '<s>', 'one', 'statistics', 'textbook', 'warns', 'thus', ':', 'none', 'of', 'the', 'null', 'hypotheses', 'we', 'have', 'considered', 'with', 'respect', 'to', 'good-', 'ness', 'of', 'fit', 'can', 'be', 'exactly', 'true', ',', 'so', 'if', 'we', 'increase', 'the', 'sample', 'size', '(', 'and', 'hence', 'the', 'value', 'of', 'χ2', ')', 'we', 'would', 'ultimately', 'reach', 'the', 'point', 'when', 'all', 'null', 'hypotheses', 'would', 'be', 'rejected', '.', '</s>', '</s>', '<s>', '<s>', 'all', 'that', 'the', 'χ2', 'test', 'can', 'tell', 'us', ',', 'then', ',', 'is', 'that', 'the', 'sample', 'size', 'is', 'too', 'small', 'to', 'reject', 'the', 'null', 'hypothesis', '!', '</s>', '</s>', '<s>', '<s>', '(', 'owen', 'and', 'jones', ',', '1977', ',', 'p', '359', ')', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'issue', 'is', 'particularly', 'salient', 'for', 'empirical', 'linguistics', 'because', ',', 'firstly', ',', 'we', 'have', 'access', 'to', 'extremely', 'large', 'sample', 'sizes', ',', 'and', 'secondly', ',', 'the', 'distri-', 'bution', 'of', 'many', 'language', 'phenomena', 'is', 'zipfian', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'has', '6,000,000', 'oc-', 'currences', 'in', 'the', 'bnc', 'whereas', 'cat', 'food', '(', 'spelled', 'as', 'one', 'word', 'or', 'two', ')', 'has', '66', '.', '</s>', '</s>', '<s>', '<s>', 'for', 'a', 'vast', 'number', 'of', 'third', 'phenomena', 'x', ',', 'the', 'null', 'hypothesis', 'that', 'the', 'and', 'x', 'are', 'uncorrelated', 'will', 'be', 'rejected', ',', 'whereas', 'the', 'null', 'hypothesis', 'that', 'cat', 'food', 'and', 'x', 'are', 'uncorrelated', 'will', 'not', '.', '</s>', '</s>', '<s>', '<s>', 'it', 'would', 'be', 'wrong', 'to', 'draw', 'inferences', 'about', 'what', 'was', 'arbitrary', ',', 'what', 'motivated', '.', '</s>', '</s>', '<s>', '<s>', '3', '.', '</s>', '</s>', '<s>', '<s>', 'objections', 'to', 'maximum', 'likelihood', 'estimates', '(', 'mles', ')', 'church', 'and', 'hanks', '(', '1990', ')', 'inaugurated', 'the', 'research', 'area', 'of', 'lexical', 'statis-', 'tics', 'with', 'their', 'presentation', 'of', 'mutual', 'information', '(', 'i', ')', ',', 'a', 'measure', 'of', 'how', 'closely', 'associated', 'two', 'phenomena', 'are', '.', '</s>', '</s>', '<s>', '<s>', 'it', 'can', 'be', 'applied', 'to', 'finding', 'words', 'which', 'occur', 'together', 'to', 'a', 'noteworthy', 'degree', ',', 'or', 'to', 'finding', 'words', 'which', 'are', 'particularly', 'associated', 'with', 'one', 'corpus', 'as', 'against', 'another', ',', 'or', 'for', 'various', 'other', 'purposes.3', 'they', 'define', 'the', 'mutual', 'information', 'between', 'two', 'words', 'x', 'and', 'y', 'as', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '267', 'i', '(', 'x', ';', 'y', ')', '⫽', 'log', '2', '冉', 'p', '(', 'xandy', ')', 'p', '(', 'x', ')', '·', 'p', '(', 'y', ')', '冊', 'and', 'then', 'estimate', 'probabilities', 'directly', 'from', 'frequencies', ',', 'that', 'is', 'using', 'the', '‘', 'maximum', 'likelihood', 'estimate', '’', '(', 'mle', ')', 'of', 'f', '(', 'x', ')', '/n', 'for', 'p', '(', 'x', ')', ',', 'f', '(', 'y', ')', '/n', 'for', 'p', '(', 'y', ')', ',', 'f', '(', 'x-and-y', ')', '/n', 'for', 'p', '(', 'x-and-y', ')', ',', 'thereby', 'giving', 'i', '(', 'x', ';', 'y', ')', '⫽', 'log', '2', '冉', 'n', '·', 'f', '(', 'x', '⫺', 'and', '⫺', 'y', ')', 'f', '(', 'x', ')', '·', 'f', '(', 'y', ')', '冊', 'dunning', '(', '1993', ')', 'presents', 'a', 'critique', 'of', 'the', 'use', 'of', 'mutual', 'information', 'in', 'empirical', 'linguistics', '.', '</s>', '</s>', '<s>', '<s>', 'his', 'objection', 'has', 'been', 'confused', 'with', 'the', 'critique', 'of', 'hypothesis-testing', 'i', 'make', 'here', 'so', 'i', 'mention', 'his', 'work', 'in', 'order', 'to', 'clarify', 'that', 'the', 'two', 'objections', ',', 'while', 'both', 'valid', ',', 'are', 'different', 'in', 'nature', 'and', 'independent', '.', '</s>', '</s>', '<s>', '<s>', 'dunning', 'demonstrates', 'how', 'mles', 'fare', 'poorly', 'when', 'estimating', 'the', 'probabilities', 'of', 'rare', 'events', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'problem', 'is', 'essentially', 'this', ':', 'if', 'a', 'word', '(', 'or', 'bigram', ',', 'or', 'trigram', ',', 'or', 'character-sequence', 'etc', '.', ')', '</s>', '</s>', '<s>', '<s>', 'occurs', 'just', 'once', 'or', 'twice', 'in', 'a', 'corpus', 'of', 'n', 'words', '(', 'bigrams', ',', 'etc', '.', '</s>', '</s>', '<s>', '<s>', ')', ',', 'then', 'the', 'simplest', 'way', 'to', 'estimate', 'the', 'probability', 'is', 'the', 'nile', ',', 'which', 'gives', '1/n', 'or', '2/n', '.', '</s>', '</s>', '<s>', '<s>', 'however', 'this', 'does', 'not', 'factor', 'in', 'the', 'arbitrariness', 'of', 'the', 'word', 'occurring', 'at', 'all', 'in', 'the', 'corpus', ':', 'in', 'a', 'corpus', 'ten', 'times', 'the', 'size', ',', 'there', 'would', 'be', 'roughly', 'ten', 'times', 'the', 'number', 'of', 'singletons', 'and', 'doubletons', 'in', 'the', 'corpus', ',', 'most', 'of', 'which', 'would', 'not', 'have', 'occurred', 'at', 'all', 'in', 'the', 'original', 'corpus', '.', '</s>', '</s>', '<s>', '<s>', 'thus', 'some', 'of', 'the', 'prob-', 'ability', 'mass', 'contributing', 'to', 'the', '1/n', 'or', '2/n', 'mles', 'should', 'have', 'been', 'put', 'aside', 'for', 'the', 'words', '(', 'bigrams', 'etc', '.', ')', '</s>', '</s>', '<s>', '<s>', 'which', 'did', 'not', 'occur', 'at', 'all', 'in', 'this', 'particular', 'corpus', '.', '</s>', '</s>', '<s>', '<s>', 'viewed', 'another', 'way', ',', 'the', '1/n', 'and', '2/n', 'should', 'be', 'dis-', 'counted', 'to', 'allow', 'for', 'the', 'fact', 'that', 'one', 'or', 'two', 'occurrences', 'are', 'very', 'low', 'bases', 'of', 'evidence', 'on', 'which', 'to', 'assert', 'probabilities', '.', '</s>', '</s>', '<s>', '<s>', 'there', 'are', 'various', 'ways', 'in', 'which', 'the', 'discounting', 'can', 'be', 'done', ',', 'for', 'example', 'the', 'good-turing', 'method', '(', 'good', '1953', ')', ',', 'usefully', 'applied', 'to', 'em-', 'pirical', 'linguistics', 'in', 'gale', 'and', 'sampson', '(', '1995', ')', ',', 'bod', '(', '1995', ')', '.', '</s>', '</s>', '<s>', '<s>', 'dunning', 'presents', 'and', 'advocates', 'the', 'use', 'of', 'the', 'log-likelihood', 'statistic', ',', 'which', ',', 'like', 'the', 'χ2', 'statistic', ',', 'is', 'χ2-distributed,4', 'but', 'more', 'accurately', 'estimates', 'probabil-', 'ities', 'where', 'counts', 'are', 'low', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'log-likelihood', 'statistic', 'still', 'only', 'estimates', 'probabilities', ':', 'since', 'dunning', '’', 's', 'work', ',', 'pedersen', '(', '1996', ')', 'has', 'shown', 'how', 'fisher', '’', 's', 'exact', 'method', 'can', 'be', 'applied', 'to', 'the', 'problem', ',', 'to', 'identify', 'the', 'exact', 'probability', 'of', 'a', 'word', '(', 'bigram', 'etc', '.', ')', '</s>', '</s>', '<s>', '<s>', 'rather', 'than', 'estimating', 'it', 'at', 'all', '.', '</s>', '</s>', '<s>', '<s>', 'thus', 'dunning', '’', 's', 'objection', 'to', 'mutual', 'information', 'is', 'that', 'it', 'fails', 'to', 'accurately', 'represent', 'probabilities', 'when', 'counts', 'are', 'low', '(', 'where', '‘', 'low', '’', 'is', 'generally', 'taken', 'as', 'less', 'than', 'five', ')', '.', '</s>', '</s>', '<s>', '<s>', 'if', 'the', 'probabilities', 'can', 'be', 'accurately', 'represented', ',', 'dunning', '’', 's', 'anxieties', 'will', 'be', 'set', 'at', 'ease', '.', '</s>', '</s>', '<s>', '<s>', '268', 'a.', 'kilgarriff', 'the', 'critique', 'in', 'this', 'paper', 'does', 'not', 'concern', 'whether', 'probabilities', 'are', 'accurately', 'calculated', '.', '</s>', '</s>', '<s>', '<s>', 'rather', ',', 'the', 'objection', 'is', 'that', 'the', 'probability', 'model', ',', 'with', 'its', 'assumptions', 'of', 'randomness', ',', 'is', 'inappropriate', ',', 'particularly', 'where', 'counts', 'are', 'high', '(', 'eg', ',', 'thousands', 'or', 'more', ')', '.', '</s>', '</s>', '<s>', '<s>', 'where', 'the', 'task', 'is', 'to', 'determine', 'whether', 'there', 'is', 'an', 'interesting', 'associa-', 'tion', 'between', 'two', 'rare', 'events', ',', 'dunning', '’', 's', 'concern', 'must', 'be', 'heeded', '.', '</s>', '</s>', '<s>', '<s>', 'where', 'it', 'is', 'to', 'determine', 'whether', 'there', 'is', 'an', 'interesting', 'association', 'between', 'high-frequency', 'events', ',', 'the', 'concerns', 'of', 'this', 'paper', 'must', 'be', '.', '</s>', '</s>', '<s>', '<s>', '4', '.', '</s>', '</s>', '<s>', '<s>', 'experiment', 'given', 'enough', 'data', ',', 'h0', 'is', 'almost', 'always', 'rejected', 'however', 'arbitrary', 'the', 'data', ',', 'as', 'the', 'author', 'discovered', 'when', 'grappling', 'with', 'the', 'following', 'data', '.', '</s>', '</s>', '<s>', '<s>', 'two', 'corpora', 'were', 'set', 'up', 'to', 'be', 'indisputably', 'of', 'the', 'same', 'language', 'type', ',', 'with', 'only', 'arbitrary', 'differences', 'between', 'them', ':', 'each', 'was', 'a', 'random', 'subset', 'of', 'the', 'written', 'part', 'of', 'the', 'british', 'national', 'corpus', '(', 'bnc', ')', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'sampling', 'was', 'as', 'follows', ':', 'all', 'texts', 'shorter', 'than', '20,000', 'words', 'were', 'excluded', '.', '</s>', '</s>', '<s>', '<s>', 'this', 'left', '820', 'texts', '.', '</s>', '</s>', '<s>', '<s>', 'half', 'the', 'texts', 'were', 'then', 'randomly', 'assigned', 'to', 'each', 'of', 'two', 'corpora', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'null', 'hypotheses', 'were', '(', '1', ')', 'that', 'the', 'two', 'subcorpora', ',', 'viewed', 'as', 'col-', 'lections', 'of', 'words', 'rather', 'than', 'documents', ',', 'were', 'random', 'samples', 'drawn', 'from', 'the', 'same', 'population', ';', 'and', 'consequently', ',', '(', '2', ')', 'that', 'the', 'deviation', 'in', 'frequency', 'of', 'occurrence', 'for', 'each', 'individual', 'word', 'between', 'the', 'two', 'sub-', 'corpora', 'was', 'explicable', 'as', 'random', 'fluctuation', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'ho', 'were', 'tested', 'using', 'the', 'χ2-test', ':', 'is', 'χ2', 'σ', '(', '|o', '⫺', 'e', '|', '⫺', '0.5', ')', '2', '/e', 'greater', 'than', 'the', 'critical', 'value', '?', '</s>', '</s>', '<s>', '<s>', 'the', 'sum', 'is', 'over', 'the', 'four', 'cells', 'of', 'the', 'contingency', 'table', 'corpus', '1', 'corpus', '2', 'word', 'w', 'a', 'b', 'not', 'word', 'w', 'c', 'd', 'if', 'we', 'randomly', 'assign', 'words', '(', 'as', 'opposed', 'to', 'documents', ')', 'to', 'the', 'one', 'corpus', 'or', 'the', 'other', ',', 'then', 'we', 'have', 'a', 'straightforward', 'random', 'distribution', ',', 'with', 'the', 'value', 'of', 'the', 'χ2-statistic', 'equal', 'to', 'or', 'greater', 'than', 'the', '99.5', '%', 'confidence', 'threshold', 'of', '7.88', 'for', 'just', '0.5', '%', 'of', 'words', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'average', 'value', 'of', 'the', 'error', 'term', ',', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '269', '(', '|o', '⫺', 'e|', '⫺', '0.5', ')', '2', '/e', 'is', 'then', '0.5.5', 'the', 'hypothesis', 'can', ',', 'therefore', ',', 'be', 'couched', 'as', ':', 'are', 'the', 'error', 'terms', 'systematically', 'greater', 'than', '0.5', '?', '</s>', '</s>', '<s>', '<s>', 'if', 'they', 'are', ',', 'we', 'should', 'be', 'wary', 'of', 'attributing', 'high', 'error', 'terms', 'to', 'significant', 'differences', 'between', 'text', 'types', ',', 'since', 'we', 'also', 'obtain', 'many', 'high', 'error', 'terms', 'where', 'there', 'are', 'no', 'significant', 'differences', 'between', 'text', 'types', '.', '</s>', '</s>', '<s>', '<s>', 'frequency', 'lists', 'for', 'word-pos', 'pairs', 'for', 'each', 'subcorpus', 'were', 'gener-', 'ated', '.', '</s>', '</s>', '<s>', '<s>', 'for', 'each', 'word', 'occurring', 'in', 'either', 'subcorpus', ',', 'the', 'error', 'term', 'which', 'would', 'have', 'contributed', 'to', 'a', 'χ2', 'calculation', 'was', 'determined', '.', '</s>', '</s>', '<s>', '<s>', 'as', 'table', '4', 'shows', ',', 'average', 'values', 'for', 'the', 'error', 'term', 'are', 'far', 'greater', 'than', '0.5', ',', 'and', 'tend', 'to', 'increase', 'as', 'word', 'frequency', 'increases', '.', '</s>', '</s>', '<s>', '<s>', 'as', 'the', 'averages', 'indicate', ',', 'the', 'error', 'term', 'is', 'very', 'often', 'greater', 'than', '0.5', '*', '7.88', '⫽', '3.94', ',', 'the', 'relevant', 'critical', 'value', 'of', 'the', 'chi-square', 'statistic', '.', '</s>', '</s>', '<s>', '<s>', 'for', 'very', 'many', 'words', ',', 'including', 'most', 'common', 'words', ',', 'the', 'null', 'hypothesis', 'is', 'resoundingly', 'defeated', '(', 'as', 'is', 'the', 'null', 'hypthesis', 'regarding', 'the', 'two', 'subc-', 'orpora', 'as', 'wholes', ')', '.', '</s>', '</s>', '<s>', '<s>', 'there', 'is', 'no', 'a', 'priori', 'reason', 'to', 'expect', 'words', 'to', 'behave', 'as', 'if', 'they', 'had', 'been', 'selected', 'at', 'random', ',', 'and', 'indeed', 'they', 'do', 'not', '.', '</s>', '</s>', '<s>', '<s>', 'it', 'is', 'in', 'the', 'nature', 'of', 'table', '1', '.', '</s>', '</s>', '<s>', '<s>', 'comparing', 'two', 'same-genre', 'corpora', 'using', 'χ2', 'class', 'first', 'item', 'in', 'class', 'mean', 'error', 'term', '(', 'words', 'in', 'freq', '.', '</s>', '</s>', '<s>', '<s>', 'order', ')', 'for', 'items', 'in', 'class', 'word', 'pos', 'first', '10', 'items', 'the', 'det', '18.76', 'next', '10', 'items', 'for', 'prp', '17.45', 'next', '20', 'items', 'not', 'xx', '14.39', 'next', '40', 'items', 'have', 'vhb', '10.71', 'next', '80', 'items', 'also', 'avo', '7.03', 'next', '160', 'items', 'know', 'vvi', '6.40', 'next', '320', 'items', 'six', 'crd', '5.30', 'next', '640', 'items', 'finally', 'av0', '6.71', 'next', '1280', 'items', 'plants', 'nn2', '6.05', 'next', '2560', 'items', 'pocket', 'nn1', '5.82', 'next', '5120', 'items', 'represent', 'vvb', '4.53', 'next', '10240', 'items', 'peking', 'np0', '3.07', 'next', '20480', 'items', 'fondly', 'av0', '1.87', 'next', '40960', 'items', 'chandelier', 'nn1', '1.15', 'table', 'note', '.', '</s>', '</s>', '<s>', '<s>', 'mean', 'error', 'term', 'is', 'far', 'greater', 'than', '0.5', ',', 'and', 'increases', 'with', 'frequency', '.', '</s>', '</s>', '<s>', '<s>', 'pos', 'tags', 'are', 'drawn', 'from', 'the', 'claws-5', 'tagset', 'as', 'used', 'in', 'the', 'bnc', '(', 'see', 'http', ':', '/info.ox', '.', '</s>', '</s>', '<s>', '<s>', 'ac.uk/bnc', ')', '270', 'a.', 'kilgarriff', 'language', 'that', 'any', 'two', 'collections', 'of', 'texts', ',', 'covering', 'a', 'wide', 'range', 'of', 'registers', '(', 'and', 'comprising', ',', 'say', ',', 'less', 'than', 'a', 'thousand', 'samples', 'of', 'over', 'a', 'thousand', 'words', 'each', ')', 'will', 'show', 'such', 'differences', '.', '</s>', '</s>', '<s>', '<s>', 'while', 'it', 'might', 'seem', 'plausible', 'that', 'oddities', 'would', 'in', 'some', 'way', 'balance', 'out', 'to', 'give', 'a', 'popula-', 'tion', 'that', 'was', 'indistinguishable', 'from', 'one', 'where', 'the', 'individual', 'words', '(', 'as', 'opposed', 'to', 'the', 'texts', ')', 'had', 'been', 'randomly', 'selected', ',', 'this', 'turns', 'out', 'not', 'to', 'be', 'the', 'case', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'key', 'word', 'in', 'the', 'last', 'paragraph', 'is', '‘', 'indistinguishable', '’', '.', '</s>', '</s>', '<s>', '<s>', 'in', 'hypothesis', 'testing', ',', 'the', 'objective', 'is', 'generally', 'to', 'see', 'if', 'the', 'population', 'can', 'be', 'distin-', 'guished', 'from', 'one', 'that', 'has', 'been', 'randomly', 'generated', '⫺', 'or', ',', 'in', 'our', 'case', ',', 'to', 'see', 'if', 'the', 'two', 'populations', 'are', 'distinguishable', 'from', 'two', 'populations', 'which', 'have', 'been', 'randomly', 'generated', 'on', 'the', 'basis', 'of', 'the', 'frequencies', 'in', 'the', 'joint', 'corpus', '.', '</s>', '</s>', '<s>', '<s>', 'since', 'words', 'in', 'a', 'text', 'are', 'not', 'random', ',', 'we', 'know', 'that', 'our', 'corpora', 'are', 'not', 'randomly', 'generated', ',', 'and', 'the', 'hypothesis', 'test', 'con-', 'firms', 'the', 'fact', '.', '</s>', '</s>', '<s>', '<s>', '5', '.', '</s>', '</s>', '<s>', '<s>', 're-analysis', 'of', 'previous', 'work', '5.1', '.', '</s>', '</s>', '<s>', '<s>', 'brown', 'and', 'lob', 'hofland', 'and', 'johansson', '(', '1982', ')', 'wanted', 'to', 'find', 'words', 'which', 'were', 'signifi-', 'cantly', 'different', 'in', 'their', 'frequencies', 'between', 'british', 'and', 'american', 'eng-', 'lish', ',', 'as', 'represented', 'in', 'the', 'brown', 'corpus', 'for', 'american', 'english', 'and', 'lob', 'corpus', 'for', 'british', '.', '</s>', '</s>', '<s>', '<s>', 'for', 'each', 'word', ',', 'they', 'tested', 'the', 'null', 'hypothesis', 'that', 'the', 'difference', 'in', 'frequency', 'between', 'the', 'two', 'corpora', 'could', 'be', 'explained', 'as', 'random', 'variation', ',', 'with', 'the', 'samples', 'being', 'random', 'samples', 'from', 'the', 'same', 'source', ',', 'and', 'in', 'their', 'frequency', 'lists', ',', 'they', 'mark', 'words', 'where', 'the', 'null', 'hypothesis', 'was', 'defeated', '(', 'at', 'a', '95', ',', '99', 'or', '99.9', '%', 'confidence', 'level', ')', '.', '</s>', '</s>', '<s>', '<s>', 'looking', 'at', 'these', 'lists', 'suggests', 'that', 'virtually', 'all', 'common', 'words', 'are', 'markedly', 'different', 'in', 'their', 'levels', 'of', 'use', 'between', 'the', 'us', 'and', 'the', 'uk', ':', 'they', 'are', 'all', 'marked', 'as', 'such', '.', '</s>', '</s>', '<s>', '<s>', 'by', 'contrast', ',', 'most', 'of', 'the', 'rarer', 'marked', 'words', 'are', 'words', 'we', 'know', 'to', 'be', 'american', 'or', 'british', ',', 'or', 'to', 'refer', 'to', 'items', 'that', 'are', 'more', 'common', 'or', 'more', 'salient', 'in', 'the', 'us', 'or', 'the', 'uk', '.', '</s>', '</s>', '<s>', '<s>', 'as', 'the', 'argument', 'of', 'the', 'previous', 'section', 'explains', ',', 'most', 'of', 'the', 'marked', 'high-frequency', 'words', 'are', 'marked', 'simply', 'as', 'a', 'consequence', 'of', 'the', 'essen-', 'tially', 'non-random', 'nature', 'of', 'language', '.', '</s>', '</s>', '<s>', '<s>', 'it', 'would', 'not', 'be', 'surprising', 'for', 'a', 'high-frequency', 'word', 'marked', 'as', 'british', 'english', 'in', 'these', 'lists', 'to', 'be', 'marked', 'as', 'american', 'english', 'in', 'a', 'repeat', 'of', 'the', 'experiment', 'using', 'new', 'data', '.', '</s>', '</s>', '<s>', '<s>', 'similar', 'strategies', 'are', 'used', 'by', ',', 'and', 'a', 'similar', 'critique', 'is', 'applicable', 'to', ',', 'leech', 'and', 'fallon', '(', '1992', ')', '(', 'again', ',', 'for', 'comparing', 'lob', 'and', 'brown', ')', ',', 'ray-', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '271', 'son', ',', 'leech', ',', 'and', 'hodges', '(', '1997', ')', 'for', 'comparing', 'the', 'conversation', 'of', 'dif-', 'ferent', 'social', 'groups', ',', 'and', 'rayson', 'and', 'garside', '(', '2000', ')', 'for', 'contrasting', 'the', 'language', 'of', 'a', 'specialist', 'genre', 'with', '‘', 'general', 'language', '’', ',', 'as', 'represented', 'by', 'the', 'british', 'national', 'corpus', '.', '</s>', '</s>', '<s>', '<s>', '5.2', 'subcategorization', 'frame', '(', 'scf', ')', 'learning', 'hypothesis-testing', 'has', 'been', 'used', 'in', 'a', 'number', 'of', 'papers', 'concerning', 'the', 'automatic', 'acquisition', 'of', 'subcategorization', 'frames', '(', 'scfs', ')', 'for', 'verbs', 'from', 'corpora', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'problem', 'is', 'this', '.', '</s>', '</s>', '<s>', '<s>', 'dictionaries', ',', 'even', 'where', 'they', 'do', 'present', 'explicit', 'and', 'accurate', 'scfs', 'for', 'verbs', ',', 'are', 'not', 'complete', ':', 'they', 'do', 'not', 'pre-', 'sent', 'all', 'the', 'frames', 'for', 'each', 'verb', '.', '</s>', '</s>', '<s>', '<s>', 'this', 'gives', 'rise', 'to', 'many', 'parsing', 'errors', '.', '</s>', '</s>', '<s>', '<s>', 'researchers', 'including', 'brent', '(', '1993', ')', ',', 'briscoe', 'and', 'carroll', '(', '1997', ')', 'and', 'kor-', 'honen', '(', '2000', ')', 'have', 'developed', 'methods', 'for', 'scf', 'acquisition', '.', '</s>', '</s>', '<s>', '<s>', 'however', ',', 'their', 'methods', 'are', 'inevitably', 'noisy', ',', 'suffering', ',', 'for', 'example', ',', 'from', 'just', 'those', 'parser', 'errors', 'that', 'the', 'whole', 'process', 'is', 'designed', 'to', 'address', ',', 'and', 'they', 'do', 'not', 'wish', 'to', 'accept', 'any', 'scf', 'for', 'which', 'there', 'is', 'any', 'evidence', 'as', 'a', 'true', 'scf', 'for', 'the', 'verb', '.', '</s>', '</s>', '<s>', '<s>', 'they', 'wish', 'to', 'filter', 'out', 'those', 'scfs', 'where', 'the', 'evidence', 'is', 'not', 'strong', 'enough', '.', '</s>', '</s>', '<s>', '<s>', 'brent', 'and', 'briscoe', 'and', 'carroll', 'used', 'hypothesis', 'testing', 'to', 'this', 'end', '.', '</s>', '</s>', '<s>', '<s>', 'however', ',', 'problems', 'are', 'noted', ':', 'further', 'evaluation', 'of', 'the', 'results', '...', 'reveals', 'that', 'the', 'filtering', 'phase', 'is', 'the', 'weak', 'link', 'in', 'the', 'system', '…', 'the', 'performance', 'of', 'the', 'filter', 'for', 'classes', 'with', 'less', 'than', '10', 'exemplars', 'is', 'around', 'chance', ',', 'and', 'a', 'simple', 'heuristic', 'of', 'accepting', 'all', 'classes', 'with', 'more', 'then', '10', 'exemplars', 'would', 'have', 'pro-', 'duced', 'broadly', 'similar', 'results', 'for', 'these', 'verbs', '(', 'briscoe', 'and', 'carroll', '1997', ':', '360⫺36', ')', '.', '</s>', '</s>', '<s>', '<s>', 'korhonen', ',', 'correll', ',', 'and', 'mccarthy', '(', '2000', ')', 'explore', 'the', 'issue', 'in', 'detail', '.', '</s>', '</s>', '<s>', '<s>', 'using', 'briscoe', 'and', 'carroll', '’', 's', 'scf', 'acquisition', 'system', ',', 'they', 'explore', 'the', 'impact', 'of', 'four', 'different', 'strategies', 'for', 'filtering', 'out', 'noise', ':', 'baseline', 'no', 'filter', 'bht', 'binomial', 'hypothesis', 'test', ':', 'reject', 'the', 'scf', 'if', 'ho', 'is', 'not', 'defeated6', 'llr', 'hypothesis', 'test', 'using', 'log-likelihood', 'ratio', ':', 'reject', 'the', 'scf', 'if', 'ho', 'is', 'not', 'defeated', 'mle', 'threshold', 'based', 'on', 'the', 'relative', 'frequency', '(', 'which', 'is', 'also', 'the', 'maximum', 'likelihood', 'estimate', '(', 'mle', ')', 'of', 'the', 'probability', ')', 'of', 'the', 'verb', 'occurring', 'in', 'the', 'scf', 'given', 'the', 'verb', ',', 'with', 'the', 'thresh-', 'old', 'determined', 'empirically', '272', 'a.', 'kilgarriff', 'they', 'observe', 'mle', 'thresholding', 'produced', 'better', 'results', 'than', 'the', 'two', 'statistical', 'tests', 'used', '.', '</s>', '</s>', '<s>', '<s>', 'precision', 'improved', 'considerably', ',', 'showing', 'that', 'the', 'classes', 'occur-', 'ring', 'in', 'the', 'data', 'with', 'the', 'highest', 'frequency', 'are', 'often', 'correct', '…', 'mle', 'is', 'not', 'adept', 'at', 'finding', 'low', 'frequency', 'scfs', '…', '(', 'korhonen', ',', 'correll', ',', 'and', 'mccarthy', '2000', ':', '202', ')', 'this', 'concurs', 'with', 'the', 'theoretical', 'argument', 'above', '.', '</s>', '</s>', '<s>', '<s>', 'hypothesis', 'tests', 'are', 'inappropriate', 'for', 'the', 'task', ',', 'because', 'the', 'relations', 'between', 'verb', 'and', 'scf', 'will', 'never', 'be', 'random', 'and', 'the', 'hypothesis', 'test', 'will', 'merely', 'reject', 'the', 'null', 'hypothesis', 'wherever', 'there', 'is', 'enough', 'data', ',', 'in', 'a', 'manner', 'not', 'closely', 'corre-', 'lated', 'with', 'whether', 'the', 'scf-verb', 'link', 'is', 'motivated', '.', '</s>', '</s>', '<s>', '<s>', 'where', 'there', 'is', 'enough', 'data', ',', 'then', 'the', 'relationship', 'between', 'verb', 'and', 'scf', 'is', 'easy', 'to', 'see', 'so', 'even', 'a', 'simple', 'threshold', 'method', 'will', 'identify', 'the', 'verb', '’', 's', 'scfs', '.', '</s>', '</s>', '<s>', '<s>', 'where', 'data', 'is', 'very', 'sparse', ',', 'no', 'method', 'works', 'well', '.', '</s>', '</s>', '<s>', '<s>', 'korhonen', '(', '2000', ')', 'extends', 'this', 'line', 'of', 'work', ',', 'exploring', 'thresholding', 'methods', 'where', 'a', 'more', 'accurate', 'estimate', 'of', 'the', 'probability', 'is', 'obtained', 'by', 'using', 'data', 'from', 'semantically', 'similar', 'but', 'higher', 'frequency', 'verbs', '.', '</s>', '</s>', '<s>', '<s>', 'she', 'achieves', 'modest', 'improvements', 'over', 'the', 'baseline', 'which', 'uses', 'korhonen', ',', 'correll', 'and', 'mccarthy', '’', 's', 'mle', ',', 'particularly', 'when', 'combining', 'the', 'fre-', 'quencies', 'of', 'the', 'target', 'verb', 'and', 'its', 'semantic', 'neighbour', 'using', 'a', 'linear', 'method', 'based', 'on', 'the', 'quantity', 'of', 'evidence', 'available', 'for', 'each', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'problem', 'is', 'not', 'one', 'of', 'distinguishing', 'random', 'and', 'non-random', 'relationships', ',', 'but', 'of', 'sparseness', 'of', 'data', '.', '</s>', '</s>', '<s>', '<s>', 'where', 'the', 'data', 'is', 'not', 'sparse', ',', 'the', 'difference', 'between', 'arbitrary', 'and', 'motivated', 'connections', 'is', 'evident', 'in', 'greatly', 'differing', 'relative', 'frequencies', '.', '</s>', '</s>', '<s>', '<s>', 'this', 'makes', 'the', 'moral', 'of', 'the', 'story', 'plain', '.', '</s>', '</s>', '<s>', '<s>', 'data', 'is', 'abundant', '.', '</s>', '</s>', '<s>', '<s>', 'a', 'modest-frequency', 'verb', 'like', 'devastate', 'occurs', '(', 'google', 'tells', 'us', ')', 'in', 'well', 'over', 'a', 'million', 'web', 'pages', '.', '</s>', '</s>', '<s>', '<s>', 'with', 'just', '1', '%', 'of', 'them', ',', 'devastate', 'becomes', 'one', 'of', 'the', 'verbs', 'for', 'which', 'we', 'have', 'plenty', 'of', 'data', ',', 'and', 'crude', 'thresholding', 'methods', 'will', 'distinguish', 'associated', 'scfs', 'from', 'noise', '.', '</s>', '</s>', '<s>', '<s>', 'it', 'is', 'possible', 'that', 'parsing', 'errors', 'are', 'systematic', 'and', 'thus', 'that', 'the', 'same', 'errors', 'occur', 'very', 'often', 'in', 'very', 'large', 'corpora', 'although', 'our', 'experi-', 'ence', 'from', 'looking', 'at', 'large', 'corpora', 'in', 'the', 'word', 'sketch', 'engine', '(', 'kilgarriff', 'et', 'al', '2004', ')', 'suggests', 'not', '.', '</s>', '</s>', '<s>', '<s>', 'harvesting', 'the', 'web', '(', 'or', 'other', 'huge', 'corpora', ')', 'is', 'the', 'way', 'to', 'build', 'an', 'accurate', 'scf', 'lexicon.7', '6', '.', '</s>', '</s>', '<s>', '<s>', 'conclusion', 'language', 'is', 'non-random', 'and', 'hence', ',', 'when', 'we', 'look', 'at', 'linguistic', 'phenom-', 'ena', 'in', 'corpora', ',', 'the', 'null', 'hypothesis', 'will', 'never', 'be', 'true', '.', '</s>', '</s>', '<s>', '<s>', 'moreover', ',', 'where', 'there', 'is', 'enough', 'data', ',', 'we', 'shall', '(', 'almost', ')', 'always', 'be', 'able', 'to', 'establish', 'that', 'it', 'is', 'not', 'true', '.', '</s>', '</s>', '<s>', '<s>', 'in', 'corpus', 'studies', ',', 'we', 'frequently', 'do', 'have', 'enough', 'data', ',', 'so', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '273', 'the', 'fact', 'that', 'a', 'relation', 'between', 'two', 'phenomena', 'is', 'demonstrably', 'non-', 'random', ',', 'does', 'not', 'support', 'the', 'inference', 'that', 'it', 'is', 'not', 'arbitrary', '.', '</s>', '</s>', '<s>', '<s>', 'hypoth-', 'esis', 'testing', 'is', 'rarely', 'useful', 'for', 'distinguishing', 'associated', 'from', 'non-associ-', 'ated', 'pairs', 'of', 'phenomena', 'in', 'large', 'corpora', '.', '</s>', '</s>', '<s>', '<s>', 'where', 'used', ',', 'it', 'has', 'often', 'led', 'to', 'unhelpful', 'or', 'misleading', 'results', '.', '</s>', '</s>', '<s>', '<s>', 'hypothesis', 'testing', 'has', 'been', 'used', 'to', 'reach', 'conclusions', ',', 'where', 'the', 'diffi-', 'culty', 'in', 'reaching', 'a', 'conclusion', 'is', 'caused', 'by', 'sparsity', 'of', 'data', '.', '</s>', '</s>', '<s>', '<s>', 'but', 'language', 'data', ',', 'in', 'this', 'age', 'of', 'information', 'glut', ',', 'is', 'available', 'in', 'vast', 'quantities', '.', '</s>', '</s>', '<s>', '<s>', 'a', 'better', 'strategy', 'will', 'generally', 'be', 'to', 'use', 'more', 'data', 'then', 'the', 'difference', 'between', 'the', 'motivated', 'and', 'the', 'arbitrary', 'will', 'be', 'evident', 'without', 'the', 'use', 'of', 'compromised', 'hypothesis', 'testing', '.', '</s>', '</s>', '<s>', '<s>', 'as', 'lord', 'rutherford', 'put', 'it', ':', '“', 'if', 'your', 'experiment', 'needs', 'statistics', ',', 'you', 'ought', 'to', 'have', 'done', 'a', 'better', 'experi-', 'ment.', '”', 'received', 'july', '2004', 'lexical', 'computing', 'ltd', '.', '</s>', '</s>', '<s>', '<s>', 'revisions', 'received', 'may', '2005', 'final', 'acceptance', 'may', '2005', 'notes', '*', 'this', 'work', 'was', 'supported', 'by', 'the', 'uk', 'epsrc', ',', 'under', 'grants', 'gr/k18931', '(', 'seal', ')', 'and', 'gr/m54971', '(', 'wasps', ')', '.', '</s>', '</s>', '<s>', '<s>', '1', '.', '</s>', '</s>', '<s>', '<s>', 'in', 'this', 'paper', 'we', 'do', 'not', 'consider', 'the', 'distinction', 'between', 'the', 'predictable', 'and', 'the', '‘', 'merely', '’', 'motivated', '.', '</s>', '</s>', '<s>', '<s>', '2', '.', '</s>', '</s>', '<s>', '<s>', 'diapers', ',', 'in', 'american', 'english', '3', '.', '</s>', '</s>', '<s>', '<s>', 'there', 'is', 'some', 'confusion', 'over', 'names', '.', '</s>', '</s>', '<s>', '<s>', 'in', 'information', 'theory', ',', 'mutual', 'information', 'is', 'usually', 'defined', 'over', 'a', 'whole', 'population', 'of', 'words', ',', 'rather', 'than', 'being', 'specified', 'for', 'a', 'particular', 'word-pair', ',', 'as', 'here', ',', 'and', 'the', 'definition', 'incorporates', 'information', 'from', 'all', 'cells', 'of', 'the', 'contingency', 'table', '.', '</s>', '</s>', '<s>', '<s>', 'church', 'and', 'hanks', 'only', 'use', 'a', 'subset', 'of', 'that', 'information', '.', '</s>', '</s>', '<s>', '<s>', 'church-and-hanks', 'mutual', 'information', 'has', 'been', 'called', 'pointwise', 'mutual', 'information', '.', '</s>', '</s>', '<s>', '<s>', 'see', 'manning', 'and', 'schütze', '(', '1999', ':', '66', 'ff', '.', ')', '</s>', '</s>', '<s>', '<s>', 'for', 'a', 'fuller', 'discus-', 'sion', '.', '</s>', '</s>', '<s>', '<s>', 'here', 'we', 'use', 'church', 'and', 'hanks', '’', 's', 'definition', 'and', 'name', '.', '</s>', '</s>', '<s>', '<s>', '4', '.', '</s>', '</s>', '<s>', '<s>', 'this', 'sentence', 'will', 'be', 'confusing', 'to', 'non-mathematicians', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'χ2', 'statistic', 'is', 'a', 'statis-', 'tic', ',', 'that', 'is', ',', 'it', 'can', 'be', 'calculated', 'from', 'a', 'data', 'sample', 'using', 'actual', 'numbers', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'χ2', 'distribution', 'is', 'a', 'theoretical', 'construct', '.', '</s>', '</s>', '<s>', '<s>', 'if', 'a', 'sufficiently', 'large', 'number', 'of', 'chi-square', 'statistics', 'are', 'calculated', ',', 'all', 'from', 'true', 'random', 'samples', 'of', 'the', 'same', 'population', ',', 'then', 'this', 'population', 'of', 'χ2', 'statistics', 'will', ',', 'provably', ',', 'fit', 'a', 'χ2', 'distribution', '.', '</s>', '</s>', '<s>', '<s>', 'this', 'is', 'also', 'true', 'for', 'other', 'statistics', ':', 'that', 'is', ',', 'if', 'a', 'sufficiently', 'large', 'number', 'of', 'log-likelihood', 'statistics', 'are', 'calculated', ',', 'all', 'from', 'true', 'random', 'samples', 'of', 'the', 'same', 'population', ',', 'then', 'this', 'population', 'of', 'log-likelihood', 'statistics', 'will', ',', 'provably', ',', 'fit', 'a', 'χ2', 'distribution', '.', '</s>', '</s>', '<s>', '<s>', 'some', 'texts', 'call', 'the', 'statistic', 'χ2', 'rather', 'than', 'χ2', 'to', 'distinguish', 'it', 'more', 'clearly', 'from', 'the', 'distribution', ',', 'but', 'this', 'practice', 'is', 'in', 'the', 'minority', 'and', 'is', 'not', 'adopted', 'here', '.', '</s>', '</s>', '<s>', '<s>', '5', '.', '</s>', '</s>', '<s>', '<s>', 'see', 'appendix', '6', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'model', 'used', 'was', 'a', 'sophisticated', 'one', 'incorporating', 'evidence', 'about', 'type', 'fre-', 'quencies', 'of', 'verbs', 'from', 'the', 'anlt', 'lexicon', ':', 'see', 'briscoe', 'and', 'carroll', '(', '1997', ')', 'or', 'kor-', 'honen', ',', 'correll', ',', 'and', 'mccarthy', '(', '2000', ')', 'for', 'details', '.', '</s>', '</s>', '<s>', '<s>', '7', '.', '</s>', '</s>', '<s>', '<s>', 'see', 'kilgarriff', 'and', 'grefenstette', '(', '2003', ')', 'and', 'papers', 'therein', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'web', 'is', 'a', 'vast', 're-', 'source', 'for', 'many', 'languages', '.', '</s>', '</s>', '<s>', '<s>', 'see', 'also', 'banko', 'and', 'brill', '(', '2001', ')', 'for', 'the', 'benefits', 'of', 'large', 'data', 'over', 'sophisticated', 'mathematics', '.', '</s>', '</s>', '<s>', '<s>', '274', 'a.', 'kilgarriff', 'appendix', 'the', 'average', 'value', 'of', 'the', 'error', 'term', 'is', '0.5', '.', '</s>', '</s>', '<s>', '<s>', 'we', 'explain', 'this', 'as', 'follows', '.', '</s>', '</s>', '<s>', '<s>', 'if', 'we', 'do', 'in', 'fact', 'have', 'a', 'random', 'distribution', ',', 'then', 'by', 'the', 'definition', 'of', 'the', 'χ2', 'distribution', ',', 'the', 'sum', 'of', 'the', 'cells', 'in', 'the', 'contingency', 'table', 'is', '1', ':', 'a⫹b⫹c⫹d⫽1', 'each', 'of', 'these', 'error', 'terms', 'is', 'calculated', 'as', '(', 'o', '⫺', 'e', '⫺', '0.5', ')', '2/e', 'in', 'our', 'situation', ',', 'there', 'are', 'very', 'large', 'datasets', 'and', 'the', 'phenomenon', 'of', 'interest', 'only', 'accounts', 'for', 'a', 'very', 'small', 'proportion', 'of', 'cases', '.', '</s>', '</s>', '<s>', '<s>', 'the', 'fre-', 'quency', 'of', 'not', 'word', 'w', 'is', 'very', 'high', '.', '</s>', '</s>', '<s>', '<s>', 'thus', 'the', 'expected', 'values', ',', 'e', ',', 'for', 'not', 'word', 'w', 'to', 'be', 'used', 'when', 'calculating', 'c', 'and', 'd', 'for', 'the', 'contingency', 'table', 'are', 'very', 'high', '.', '</s>', '</s>', '<s>', '<s>', 'as', 'we', 'divide', 'by', 'very', 'large', 'e', ',', 'c', 'and', 'd', 'are', 'vanishingly', 'small', ',', 'so', 'a⫹b⫹c⫹d⫽1', 'reduces', 'to', 'a⫹b⫽1', 'since', 'we', 'have', 'set', 'the', 'situation', 'up', 'symmetrically', ',', 'a', 'and', 'b', 'are', 'the', 'same', 'size', ',', 'so', 'each', 'will', 'be', ',', 'on', 'average', ',', '0.5', '.', '</s>', '</s>', '<s>', '<s>', 'references', 'banko', ',', 'michele', 'and', 'eric', 'brill', '2001', 'scaling', 'to', 'very', 'very', 'large', 'corpora', 'for', 'natural', 'language', 'disambiguation', '.', '</s>', '</s>', '<s>', '<s>', 'proceedings', 'of', 'the', '39th', 'annual', 'meeting', 'of', 'the', 'association', 'for', 'computa-', 'tional', 'linguistics', 'and', 'the', '10th', 'conference', 'of', 'the', 'european', 'chapter', 'of', 'the', 'association', 'for', 'computational', 'linguistics', '.', '</s>', '</s>', '<s>', '<s>', 'bod', ',', 'rens', '1995', 'enriching', 'linguistics', 'with', 'statistics', ':', 'performance', 'models', 'of', 'natural', 'lan-', 'guage', '.', '</s>', '</s>', '<s>', '<s>', 'ph.d.', 'dissertation', ',', 'university', 'of', 'amsterdam', '.', '</s>', '</s>', '<s>', '<s>', 'brandstätter', ',', 'e.', '1999', 'confidence', 'intervals', 'as', 'an', 'alternative', 'to', 'significance', 'testing', '.', '</s>', '</s>', '<s>', '<s>', 'methods', 'of', 'psychological', 'research', 'outline', '4', '(', '2', ')', ',', '33⫺46', '.', '</s>', '</s>', '<s>', '<s>', 'brent', ',', 'michael', 'r.', '1993', 'from', 'grammar', 'to', 'lexicon', ':', 'unsupervised', 'learning', 'of', 'lexical', 'syntax', '.', '</s>', '</s>', '<s>', '<s>', 'com-', 'putational', 'linguistics', '19', '(', '2', ')', ',', '243⫺262', '.', '</s>', '</s>', '<s>', '<s>', 'briscoe', ',', 'ted', 'and', 'john', 'carroll', '1997', 'automatic', 'extraction', 'of', 'subcategorization', 'from', 'corpora', '.', '</s>', '</s>', '<s>', '<s>', 'proceedings', 'of', 'the', 'fifth', 'conference', 'on', 'applied', 'natural', 'language', 'processing', ',', '356⫺363', '.', '</s>', '</s>', '<s>', '<s>', 'language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', '275', 'carver', ',', 'r.', 'p.', '1993', 'the', 'case', 'against', 'statistical', 'significance', 'testing', ',', 'revisited', '.', '</s>', '</s>', '<s>', '<s>', 'journal', 'of', 'ex-', 'perimental', 'education', '61', ',', '287⫺292', '.', '</s>', '</s>', '<s>', '<s>', 'church', ',', 'kenneth', 'and', 'patrick', 'hanks', '1990', 'word', 'association', 'norms', ',', 'mutual', 'information', 'and', 'lexicography', '.', '</s>', '</s>', '<s>', '<s>', 'compu-', 'tational', 'linguistics', '16', '(', '1', ')', ',', '22⫺29', '.', '</s>', '</s>', '<s>', '<s>', 'dunning', ',', 'ted', '1993', 'accurate', 'methods', 'for', 'the', 'statistics', 'of', 'surprise', 'and', 'coincidence', '.', '</s>', '</s>', '<s>', '<s>', 'compu-', 'tational', 'linguistics', '19', '(', '1', ')', ',', '61⫺74', '.', '</s>', '</s>', '<s>', '<s>', 'gale', ',', 'william', 'and', 'geoffrey', 'sampson', '1995', 'good-turing', 'frequency', 'estimation', 'without', 'tears', '.', '</s>', '</s>', '<s>', '<s>', 'journal', 'of', 'quantitative', 'linguistics', '2', '(', '3', ')', ',', 'good', ',', 'i.', 'j', '.', '</s>', '</s>', '<s>', '<s>', '1953', 'the', 'population', 'frequencies', 'of', 'species', 'and', 'the', 'estimation', 'of', 'population', 'parameters', '.', '</s>', '</s>', '<s>', '<s>', 'biometrika', '40', ',', '237⫺264', '.', '</s>', '</s>', '<s>', '<s>', 'grefenstette', ',', 'gregory', 'and', 'julien', 'nioche', '2000', 'estimation', 'of', 'english', 'and', 'non-english', 'language', 'use', 'on', 'the', 'www', '.', '</s>', '</s>', '<s>', '<s>', 'in', 'proceedings', 'of', 'riao', '(', 'recherche', 'd', '’', 'informations', 'assiste´', 'e', 'par', 'ordinateur', ')', ',', '237⫺246', '.', '</s>', '</s>', '<s>', '<s>', 'hofland', ',', 'knud', 'and', 'stig', 'johanson', '(', 'eds', '.', ')', '</s>', '</s>', '<s>', '<s>', '1982', 'word', 'frequencies', 'in', 'british', 'and', 'american', 'english', '.', '</s>', '</s>', '<s>', '<s>', 'bergen', ':', 'the', 'norwe-', 'gian', 'computing', 'centre', 'for', 'the', 'humanities', '.', '</s>', '</s>', '<s>', '<s>', 'korhonen', ',', 'anna', '2000', 'using', 'semantically', 'motivated', 'estimates', 'to', 'help', 'subcategorization', 'acquisition', '.', '</s>', '</s>', '<s>', '<s>', 'proceedings', 'of', 'the', 'joint', 'conference', 'on', 'empirical', 'methods', 'in', 'nlp', 'and', 'very', 'large', 'corpora', ',', '216⫺223', '.', '</s>', '</s>', '<s>', '<s>', 'korhonen', ',', 'anna', ',', 'genevieve', 'gorrell', ',', 'and', 'diana', 'mccarthy', '2000', 'statistical', 'filtering', 'and', 'subcategorization', 'frame', 'acquisition', '.', '</s>', '</s>', '<s>', '<s>', 'proceedings', 'of', 'the', 'joint', 'conference', 'on', 'empirical', 'methods', 'in', 'nlp', 'and', 'very', 'large', 'corpora', ',', '199⫺206', '.', '</s>', '</s>', '<s>', '<s>', 'ldoce', '1995', 'longman', 'dictionary', 'of', 'contemporary', 'english', ',', '3rd', 'edition', '.', '</s>', '</s>', '<s>', '<s>', 'ed', '.', '</s>', '</s>', '<s>', '<s>', 'della', 'summers', '.', '</s>', '</s>', '<s>', '<s>', 'harlow', ':', 'longman', '.', '</s>', '</s>', '<s>', '<s>', 'leech', ',', 'geoffrey', 'and', 'roger', 'fallon', '1992', 'computer', 'corpora', '—', 'what', 'do', 'they', 'tell', 'us', 'about', 'culture', '?', '</s>', '</s>', '<s>', '<s>', 'icame', 'journal', '16', ',', '29⫺50', '.', '</s>', '</s>', '<s>', '<s>', 'manning', ',', 'christopher', 'and', 'hinrich', 'schütze', '1999', 'foundations', 'of', 'statistical', 'natural', 'language', 'processing', '.', '</s>', '</s>', '<s>', '<s>', 'cambridge', ',', 'ma', ':', 'mit', 'press', '.', '</s>', '</s>', '<s>', '<s>', 'owen', ',', 'frank', 'and', 'ronald', 'jones', '1977', 'statistics', '.', '</s>', '</s>', '<s>', '<s>', 'polytech', 'publishers', '.', '</s>', '</s>', '<s>', '<s>', 'pedersen', ',', 'ted', '1996', 'fishing', 'for', 'exactness', '.', '</s>', '</s>', '<s>', '<s>', 'proceedings', 'of', 'the', 'conference', 'of', 'the', 'south-central', 'sas', 'users', 'group', ',', '188⫺200', '.', '</s>', '</s>', '<s>', '<s>', 'rayson', ',', 'paul', 'and', 'roger', 'garside', '2000', 'comparing', 'corpora', 'using', 'frequency', 'profiling', '.', '</s>', '</s>', '<s>', '<s>', 'proceedings', 'of', 'the', 'work-', 'shop', 'on', 'comparing', 'corpora', ',', '38th', 'acl', ',', '1⫺6', '.', '</s>', '</s>', '<s>', '<s>', 'rayson', ',', 'paul', ',', 'geoffrey', 'leech', ',', 'and', 'mary', 'hodges', '1997', 'social', 'differentiation', 'in', 'the', 'use', 'of', 'english', 'vocabulary', ':', 'some', 'analysis', 'of', 'the', 'conversational', 'component', 'of', 'the', 'british', 'national', 'corpus', '.', '</s>', '</s>', '<s>', '<s>', 'interna-', 'tional', 'journal', 'of', 'corpus', 'linguistics', '2', '(', '1', ')', ',', '133⫺152', '.', '</s>', '</s>', '<s>', '<s>', 'stubbs', ',', 'michael', '1995', 'collocations', 'and', 'semantic', 'profiles', ':', 'on', 'the', 'cause', 'of', 'the', 'trouble', 'with', 'quantitative', 'studies', '.', '</s>', '</s>', '<s>', '<s>', 'functions', 'of', 'language', '2', '(', '1', ')', ',', '23⫺55', '.', '</s>', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Training the corpus:***"
      ],
      "metadata": {
        "id": "q74XTy33I_Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE\n",
        "\n",
        "trained, padded = padded_everygram_pipeline(n, tokenized_corpus)\n",
        "\n",
        "#using Maximum Likelihood Estimater(MLE) to train an n-gram model\n",
        "#with a user-defined n-gram size\n",
        "model = MLE(n)\n",
        "model.fit(trained, padded)\n",
        "\n",
        "#check if corpus is trained\n",
        "print(model.counts)\n",
        "print(model.vocab)\n",
        "print(model.vocab.lookup(tokenized_corpus[0]))\n",
        "\n",
        "#testing whether a particular word is in the corpus\n",
        "#automatically replace words not in the vocabulary with '<UNK>'\n",
        "print(\"\\nTesting if a word is in the corpus: \")\n",
        "word = 'lah'\n",
        "sentence = 'language is never random ' + word + ' .'\n",
        "print(model.vocab.lookup(sentence.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiXlVl0KHbX4",
        "outputId": "812a48a1-a8fd-48ee-d359-6478cead54e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 19611 ngrams>\n",
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1391 items>\n",
            "('language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.')\n",
            "\n",
            "Testing if a word is in the corpus: \n",
            "('language', 'is', 'never', 'random', '<UNK>', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Using the trained model and computing a matrix of probabilities:***"
      ],
      "metadata": {
        "id": "zpESgdmXQBPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "from tkinter.constants import Y\n",
        "\n",
        "n = n\n",
        "n_grams = list(zip(ngrams(flattened_text, n)))\n",
        "\n",
        "#calculate the probability\n",
        "def probability(of, given):\n",
        "    p = round(float(model.score(of, given.split())), 4)\n",
        "    return p\n",
        "\n",
        "#for convention to look neat\n",
        "def display(string, from_conv, to_conv, x, y):\n",
        "    space = from_conv\n",
        "    if (n >= 3):\n",
        "        space = to_conv\n",
        "    if (x < y):\n",
        "        return string + space\n",
        "    return string\n",
        "\n",
        "#for a matrix to get it's transpose - alternatively np.transpose(matrix)\n",
        "def transpose(m):\n",
        "    return [[m[j][i] for j in range(len(m))] for i in range(len(m[0]))]\n",
        "\n",
        "stringy = []\n",
        "\n",
        "#build the language model using a matrix\n",
        "def table(change):\n",
        "    matrix = []\n",
        "    probabilities = []\n",
        "    rows, columns = (len(n_grams) - 1, len(n_grams) - 1)\n",
        "\n",
        "    for i in range(rows):\n",
        "        given = ''\n",
        "        column = []\n",
        "        clone = []\n",
        "        of = n_grams[i][0][change]\n",
        "\n",
        "        for k in range(change):\n",
        "            given = display(given + n_grams[i][0][k], '', ' ', k, change - 1)\n",
        "        column.append(given)\n",
        "\n",
        "        for j in range(columns):\n",
        "            if (i < 1):\n",
        "                column.append(n_grams[j + 1][0][change])\n",
        "            else:\n",
        "                if (j > 0):\n",
        "                    given = ''\n",
        "                    for y in range(change):\n",
        "                        given = display(given + n_grams[j][0][y], '', ' ', y, change - 1)\n",
        "                    stringy.append('P(\\'' + of + '\\'|\\'' + given + '\\') = ' + str(probability(of, given)))\n",
        "                    column.append(str(probability(of, given)))\n",
        "                    clone.append(str(probability(of, given)))\n",
        "        stringy.append(\"\")\n",
        "        matrix.append(column)\n",
        "        probabilities.append(clone)\n",
        "\n",
        "    print(\"\\nLanguage Model:\")\n",
        "    matrix[0][0] = ''\n",
        "    matrix = np.array(matrix, dtype=object)\n",
        "    probabilities = np.array(probabilities, dtype=object)\n",
        "    probabilities = np.delete(probabilities, 0)\n",
        "    probabilities = transpose(probabilities)\n",
        "\n",
        "    for row in range(rows):\n",
        "        for col in range(columns):\n",
        "            if (row > 0 and col > 0):\n",
        "                matrix[row][col] = probabilities[row-1][col-1]\n",
        "\n",
        "    print(tabulate(matrix, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "metadata": {
        "id": "4ysfavAnQAXV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Probability calculation(s) and checking: \")\n",
        "table(n - 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtZ1K87fS5TW",
        "outputId": "067d1be7-00cd-4a87-e95c-f622fed52e16"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability calculation(s) and checking: \n",
            "\n",
            "Language Model:\n",
            "╒════════════════════════╤═════════╤═════════╤══════════╤═════════╤════════════╤═════╤═══════╤════════════╤══════╤═══════════════╤══════════════╤═════╤════════╕\n",
            "│                        │   users │   never │   choose │   words │   randomly │   , │   and │   language │   is │   essentially │   non-random │   . │   </s> │\n",
            "╞════════════════════════╪═════════╪═════════╪══════════╪═════════╪════════════╪═════╪═══════╪════════════╪══════╪═══════════════╪══════════════╪═════╪════════╡\n",
            "│ <s> Language           │       0 │  0      │        0 │       0 │          0 │   0 │     0 │       0    │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ Language users         │       0 │  0      │        0 │       0 │          0 │   0 │     0 │       0    │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ users never            │       0 │  0      │        1 │       0 │          0 │   0 │     0 │       0    │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ never choose           │       0 │  0      │        0 │       1 │          0 │   0 │     0 │       0    │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ choose words           │       0 │  0      │        0 │       0 │          1 │   0 │     0 │       0    │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ words randomly         │       0 │  0      │        0 │       0 │          0 │   1 │     0 │       0    │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ randomly ,             │       0 │  0      │        0 │       0 │          0 │   0 │     1 │       0    │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ , and                  │       0 │  0      │        0 │       0 │          0 │   0 │     0 │       0.04 │    0 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ and language           │       0 │  0      │        0 │       0 │          0 │   0 │     0 │       0    │    1 │        0      │       0      │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ language is            │       0 │  0.6364 │        0 │       0 │          0 │   0 │     0 │       0    │    0 │        0.0909 │       0.0909 │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ is essentially         │       0 │  0      │        0 │       0 │          0 │   0 │     0 │       0    │    0 │        0      │       0.5    │   0 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ essentially non-random │       0 │  0      │        0 │       0 │          0 │   0 │     0 │       0    │    0 │        0      │       0      │   1 │      0 │\n",
            "├────────────────────────┼─────────┼─────────┼──────────┼─────────┼────────────┼─────┼───────┼────────────┼──────┼───────────────┼──────────────┼─────┼────────┤\n",
            "│ non-random .           │       0 │  0      │        0 │       0 │          0 │   0 │     0 │       0    │    0 │        0      │       0      │   0 │      1 │\n",
            "╘════════════════════════╧═════════╧═════════╧══════════╧═════════╧════════════╧═════╧═══════╧════════════╧══════╧═══════════════╧══════════════╧═════╧════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(stringy)):\n",
        "    print(stringy[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpAhgnbcUVzH",
        "outputId": "b9b775a0-45f6-4ceb-e44e-8480477dac81"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "P('users'|'<s> Language') = 0.0\n",
            "P('users'|'Language users') = 0.0\n",
            "P('users'|'users never') = 0.0\n",
            "P('users'|'never choose') = 0.0\n",
            "P('users'|'choose words') = 0.0\n",
            "P('users'|'words randomly') = 0.0\n",
            "P('users'|'randomly ,') = 0.0\n",
            "P('users'|', and') = 0.0\n",
            "P('users'|'and language') = 0.0\n",
            "P('users'|'language is') = 0.0\n",
            "P('users'|'is essentially') = 0.0\n",
            "P('users'|'essentially non-random') = 0.0\n",
            "P('users'|'non-random .') = 0.0\n",
            "\n",
            "P('never'|'<s> Language') = 0.0\n",
            "P('never'|'Language users') = 0.0\n",
            "P('never'|'users never') = 0.0\n",
            "P('never'|'never choose') = 0.0\n",
            "P('never'|'choose words') = 0.0\n",
            "P('never'|'words randomly') = 0.0\n",
            "P('never'|'randomly ,') = 0.0\n",
            "P('never'|', and') = 0.0\n",
            "P('never'|'and language') = 0.0\n",
            "P('never'|'language is') = 0.6364\n",
            "P('never'|'is essentially') = 0.0\n",
            "P('never'|'essentially non-random') = 0.0\n",
            "P('never'|'non-random .') = 0.0\n",
            "\n",
            "P('choose'|'<s> Language') = 0.0\n",
            "P('choose'|'Language users') = 0.0\n",
            "P('choose'|'users never') = 1.0\n",
            "P('choose'|'never choose') = 0.0\n",
            "P('choose'|'choose words') = 0.0\n",
            "P('choose'|'words randomly') = 0.0\n",
            "P('choose'|'randomly ,') = 0.0\n",
            "P('choose'|', and') = 0.0\n",
            "P('choose'|'and language') = 0.0\n",
            "P('choose'|'language is') = 0.0\n",
            "P('choose'|'is essentially') = 0.0\n",
            "P('choose'|'essentially non-random') = 0.0\n",
            "P('choose'|'non-random .') = 0.0\n",
            "\n",
            "P('words'|'<s> Language') = 0.0\n",
            "P('words'|'Language users') = 0.0\n",
            "P('words'|'users never') = 0.0\n",
            "P('words'|'never choose') = 1.0\n",
            "P('words'|'choose words') = 0.0\n",
            "P('words'|'words randomly') = 0.0\n",
            "P('words'|'randomly ,') = 0.0\n",
            "P('words'|', and') = 0.0\n",
            "P('words'|'and language') = 0.0\n",
            "P('words'|'language is') = 0.0\n",
            "P('words'|'is essentially') = 0.0\n",
            "P('words'|'essentially non-random') = 0.0\n",
            "P('words'|'non-random .') = 0.0\n",
            "\n",
            "P('randomly'|'<s> Language') = 0.0\n",
            "P('randomly'|'Language users') = 0.0\n",
            "P('randomly'|'users never') = 0.0\n",
            "P('randomly'|'never choose') = 0.0\n",
            "P('randomly'|'choose words') = 1.0\n",
            "P('randomly'|'words randomly') = 0.0\n",
            "P('randomly'|'randomly ,') = 0.0\n",
            "P('randomly'|', and') = 0.0\n",
            "P('randomly'|'and language') = 0.0\n",
            "P('randomly'|'language is') = 0.0\n",
            "P('randomly'|'is essentially') = 0.0\n",
            "P('randomly'|'essentially non-random') = 0.0\n",
            "P('randomly'|'non-random .') = 0.0\n",
            "\n",
            "P(','|'<s> Language') = 0.0\n",
            "P(','|'Language users') = 0.0\n",
            "P(','|'users never') = 0.0\n",
            "P(','|'never choose') = 0.0\n",
            "P(','|'choose words') = 0.0\n",
            "P(','|'words randomly') = 1.0\n",
            "P(','|'randomly ,') = 0.0\n",
            "P(','|', and') = 0.0\n",
            "P(','|'and language') = 0.0\n",
            "P(','|'language is') = 0.0\n",
            "P(','|'is essentially') = 0.0\n",
            "P(','|'essentially non-random') = 0.0\n",
            "P(','|'non-random .') = 0.0\n",
            "\n",
            "P('and'|'<s> Language') = 0.0\n",
            "P('and'|'Language users') = 0.0\n",
            "P('and'|'users never') = 0.0\n",
            "P('and'|'never choose') = 0.0\n",
            "P('and'|'choose words') = 0.0\n",
            "P('and'|'words randomly') = 0.0\n",
            "P('and'|'randomly ,') = 1.0\n",
            "P('and'|', and') = 0.0\n",
            "P('and'|'and language') = 0.0\n",
            "P('and'|'language is') = 0.0\n",
            "P('and'|'is essentially') = 0.0\n",
            "P('and'|'essentially non-random') = 0.0\n",
            "P('and'|'non-random .') = 0.0\n",
            "\n",
            "P('language'|'<s> Language') = 0.0\n",
            "P('language'|'Language users') = 0.0\n",
            "P('language'|'users never') = 0.0\n",
            "P('language'|'never choose') = 0.0\n",
            "P('language'|'choose words') = 0.0\n",
            "P('language'|'words randomly') = 0.0\n",
            "P('language'|'randomly ,') = 0.0\n",
            "P('language'|', and') = 0.04\n",
            "P('language'|'and language') = 0.0\n",
            "P('language'|'language is') = 0.0\n",
            "P('language'|'is essentially') = 0.0\n",
            "P('language'|'essentially non-random') = 0.0\n",
            "P('language'|'non-random .') = 0.0\n",
            "\n",
            "P('is'|'<s> Language') = 0.0\n",
            "P('is'|'Language users') = 0.0\n",
            "P('is'|'users never') = 0.0\n",
            "P('is'|'never choose') = 0.0\n",
            "P('is'|'choose words') = 0.0\n",
            "P('is'|'words randomly') = 0.0\n",
            "P('is'|'randomly ,') = 0.0\n",
            "P('is'|', and') = 0.0\n",
            "P('is'|'and language') = 1.0\n",
            "P('is'|'language is') = 0.0\n",
            "P('is'|'is essentially') = 0.0\n",
            "P('is'|'essentially non-random') = 0.0\n",
            "P('is'|'non-random .') = 0.0\n",
            "\n",
            "P('essentially'|'<s> Language') = 0.0\n",
            "P('essentially'|'Language users') = 0.0\n",
            "P('essentially'|'users never') = 0.0\n",
            "P('essentially'|'never choose') = 0.0\n",
            "P('essentially'|'choose words') = 0.0\n",
            "P('essentially'|'words randomly') = 0.0\n",
            "P('essentially'|'randomly ,') = 0.0\n",
            "P('essentially'|', and') = 0.0\n",
            "P('essentially'|'and language') = 0.0\n",
            "P('essentially'|'language is') = 0.0909\n",
            "P('essentially'|'is essentially') = 0.0\n",
            "P('essentially'|'essentially non-random') = 0.0\n",
            "P('essentially'|'non-random .') = 0.0\n",
            "\n",
            "P('non-random'|'<s> Language') = 0.0\n",
            "P('non-random'|'Language users') = 0.0\n",
            "P('non-random'|'users never') = 0.0\n",
            "P('non-random'|'never choose') = 0.0\n",
            "P('non-random'|'choose words') = 0.0\n",
            "P('non-random'|'words randomly') = 0.0\n",
            "P('non-random'|'randomly ,') = 0.0\n",
            "P('non-random'|', and') = 0.0\n",
            "P('non-random'|'and language') = 0.0\n",
            "P('non-random'|'language is') = 0.0909\n",
            "P('non-random'|'is essentially') = 0.5\n",
            "P('non-random'|'essentially non-random') = 0.0\n",
            "P('non-random'|'non-random .') = 0.0\n",
            "\n",
            "P('.'|'<s> Language') = 0.0\n",
            "P('.'|'Language users') = 0.0\n",
            "P('.'|'users never') = 0.0\n",
            "P('.'|'never choose') = 0.0\n",
            "P('.'|'choose words') = 0.0\n",
            "P('.'|'words randomly') = 0.0\n",
            "P('.'|'randomly ,') = 0.0\n",
            "P('.'|', and') = 0.0\n",
            "P('.'|'and language') = 0.0\n",
            "P('.'|'language is') = 0.0\n",
            "P('.'|'is essentially') = 0.0\n",
            "P('.'|'essentially non-random') = 1.0\n",
            "P('.'|'non-random .') = 0.0\n",
            "\n",
            "P('</s>'|'<s> Language') = 0.0\n",
            "P('</s>'|'Language users') = 0.0\n",
            "P('</s>'|'users never') = 0.0\n",
            "P('</s>'|'never choose') = 0.0\n",
            "P('</s>'|'choose words') = 0.0\n",
            "P('</s>'|'words randomly') = 0.0\n",
            "P('</s>'|'randomly ,') = 0.0\n",
            "P('</s>'|', and') = 0.0\n",
            "P('</s>'|'and language') = 0.0\n",
            "P('</s>'|'language is') = 0.0\n",
            "P('</s>'|'is essentially') = 0.0\n",
            "P('</s>'|'essentially non-random') = 0.0\n",
            "P('</s>'|'non-random .') = 1.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
